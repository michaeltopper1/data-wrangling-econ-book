--- 
title: "Data Wrangling for Economists"
author: 
  - Michael Topper^[UC Santa Barbara, michaeltopper@ucsb.edu]
  - Danny Klinenberg^[UC Santa Barbara, dklinenberg@ucsb.edu]
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
# output: bookdown::gitbook
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: openscapes/series
description: "Enter description."
---

# Welcome {#welcome}

This book is designed to go with the UCSB course Econ 245: Data Wrangling for Economists. The book is still a work in progress and will be updated regularly.

In the following chapters, you will practice data wrangling with some hand-holding as well as some simple exercises to check your understanding. The purpose of these chapters is the following:

1. Make your homework easier.
2. Make you a better coder by observing good code.
3. Get hands-on practice with functions you will need to complete your assignments.
4. Provide a reference for each topic in the course.


It is *strongly* suggested that you code along with the examples that we go through, although there will be times when copying and pasting can be useful (e.g., loading in a data set). 

## Organization

The book's organization was designed to help the second-year PhD student get ready to do research as soon as possible. There is focus on both organization and coding techniques. In later updates, we hope to add in some more advanced topics that are covered in the course such as PDF extracting, webscraping, estimation, making a package, publication tables, and spatial data. The book was made with a `tidyverse` focus.

<!--chapter:end:index.Rmd-->


# Intro to R

In this chapter, we will be focusing on two topics. The first will be doing common statistics in R, while the other will be an introduction to vectors, tibbles, and logical operators. 

To begin, we will first load in the data sets that we want to use. We will be using the `titanic_train` data set from the `titanic` package. The `titanic_train` data set provides information on the fate of passengers on the fatal maiden voyage of the ocean liner "Titanic", summarized according to economic status (class), sex, age and survival. Some import columns:

* `Survived`: binary variable equal to 1 if the passenger survived
* `Pclass`: the passenger's class
* `name`: the passenger's name
* `Sex`: the sex of the passenger
* `Age`: the age of the passenger

Let's load in the data set along with the packages we will be using. Quick aside: A package is a set of functions that are not defined in R by default. Many people create their own packages which have their own unique functions and data sets within. Packages are easy to install, and you only have to install a package one time and then it will be in your local machine for future use. We can install packages using the `install.packages` function. Important: installing a package **does not** mean it has been loaded. To be able to use the functions and data sets within the package, you must load the package by using the `library` function. Also important: a package needs to be loaded within each R session Hence, if you restart R or quit and reopen RStudio (which will restart R), you will need to re-load your packages. Observe:  
```{r, eval = F}
## installing the necessary packages. The titanic package has the data set we want
## the tidyverse package has the functions we want
install.packages("titanic")
install.packages("tidyverse")
```

```{r}
## loading in the tidyverse and titanic packages
## remember: functions and data in the packages cannot be used unless loaded in!
## this must be done in each R session!
library(tidyverse)
library(titanic)

## loading in the titanic data set
titanic <- titanic_train
```

## Basic summary statistics

To start off, we will go through some basic summary statistics. For this section, we will focus on the `Age` column of the data set. 

Let's say we wanted to view only the `Age` column of the data. We could accomplish this using the `$` attachment. Observe:

```{r, eval = F}
## viewing only the age column
titanic$Age
```

From this, we can see that there are many values in the `Age` column, some numbers, and others NA. NA is a value that R recognizes as "missing". We will touch more on this in lectures and other assignments. For now, just think of it as a missing element. 

Alternatively, we can get a "snapshot" of our data using the `head` function. 

```{r}
## using the head function to get a snapshot of the data
head(titanic)
```

Taking a look at your data before you start doing analysis is imperative, and the `head` function gives you a concise preview of most of your columns. 

Now, let's attempt some basic summary statistics. For starters, we will use the `mean`, `sd`, and `var` functions to get the mean, standard deviation, and variance of the `Age` column respectively. 

```{r}
## finding the mean of the age column
mean(titanic$Age)
```

We get a value of NA as our mean of the `Age` column, despite our knowledge that the `Age` column contains many different age values. Let's use the `?` to get more information on the `mean` function to see what may be causing this. 

```{r, eval = F}
?mean
```

If we look at some of the arguments that the `mean` function takes, we can see that one of them is called `na.rm`. The `na.rm` function is initially defaulted to `FALSE`. But what does `na.rm` do? If we set the `na.rm` argument to `TRUE`, then `na.rm` will remove the NA values before the computation proceeds. This is exactly what we need to get a numerical answer for our mean. Before, R was trying to compute missing values into our `mean` function which does not make any sense (hence the nonsensical answer).  

```{r}
## trying this again using the na.rm = T argument
mean(titanic$Age, na.rm = T)
```

Now we finally get our desired result. It is important to check your column values using the `head` function if you ever get NA as a statistic to make sure your whole column is not solely missing values. Now it's time to try it yourself.

### Exercise 

* Using the `sd` and `var` functions, find the standard deviation and variance of the `Age` column. Check your solutions at the end of the document. 

## Basic histogram

Another tool to give you an idea of the distribution of your data is the `hist` function. The `hist` function creates a histogram of your data. Suppose we are interested in the distribution of ages on the titanic. We could quickly get an accurate depiction by graphing a histogram of the column.

```{r}
## graphing a histogram of the Age column
hist(titanic$Age)
```

### Exercise 

* Make a histogram of the `Fare` column. Change the title of the histogram to "Distribution of Fare". Do you notice anything interesting?

## Creating Vectors 

R is a *vectorized* computer language. This means that when you perform a function, it performs it on an entire vector of values, rather than value-by-value. The vectorization of R allows us to perform functions on an entire column of the spreadsheet, rather than going cell-by-cell. In the previous example with the `titanic` data set, we took the mean of an entire column of values by simply using the `mean` function rather than iterating through each element of our column, adding up the values, and dividing by the total. 

To get a better understanding of vectors, it is useful to create them yourself. Let's create a vector using the `c` function. To create a vector, we need a few things:

1. If we want to refer to our vector later, we need to name it.
2. We need to give the vector elements.

To begin, we will investigate (1). 
```{r}
## creating a vector but not saving it
## the vector has 4 elements
c(1,3,4,10)
```

Here, we created a vector of four values: 1, 3, 4, and 10. Notice at the top right of RStudio that our Global Environment did not change. This is because we did not save the vector. To save the vector, we need to give it a name. For our purposes, the vector will be named `first_vector`. 

```{r}
## creating a vector and naming it to save
first_vector <- c(1,3,4,10)
```

Once you run this line, you should see that your Global Environment has changed. It should now have the `first_vector` along with a small description of what it contains (e.g. 1, 3, 4, 10). The purpose of saving vectors (or saving anything) is so that we can refer to it later. Instead of creating a new vector of 1, 3, 4, and 10 each time we want to use it, we can *call* it by typing its name.

```{r}
## calling our vector to observe its elements
first_vector
```


### Exercise 

* Create two vectors called `my_own_vector1` and `my_own_vector2`. Each vector should have four numbers in them. Use whichever numbers you like. 


## Comparing Vectors

Vectors can be compared to values and other vectors. To perform this task we use *logical operators*. The *logical operators* are similar to what you learned in elementary math: greater than, equal to, or not equal to. In R, we use the following for logical operators for comparisons:

* `<` greater (less) than
* `<=` greater (less) than or equal to 
* `==` equal to
* `!=` not equal to

The best way to understand how logical operators work is to see them in action. As an example, we will compare our `first_vector` to the value 7.  

```{r}
## which elements are greater than 7?
7 > first_vector
```

```{r}
## ordering does not matter
first_vector < 7
```

Notice that R returned logical values TRUE and FALSE. R went through each element of our vector and checked whether the element was less than the number 7. According to our output, the first three elements in our vector were less than the number 7 (hence TRUE) and the last element was not (hence FALSE). We can evaluate the other logical operators as well:

```{r}
## checking if each element is not equal to 7
first_vector != 7
```

```{r}
## checking if each element is equal to 7
first_vector == 7
```

As a brief aside, observe the last line of code we wrote to check whether `first_vector` was equal to 7. Notice that the "equal to" operator is `==` rather than `=`. This is because `==` checks the equivalence of components, while `=`  is an *assignment* character similar to `<-`. In fact, `<-` and `=` serve the exact same purpose for assignment. 

```{r}
## assigning our first_vector to the value 7
## we are not checking whether it is equal to 7
first_vector = 7

## showing what our first_vector contains after assignment
first_vector
```

The R community typically uses `<-` as assignment rather than `=`. It does not matter which one you use when assigning a variable name, but this class will use the `<-` assignment as it is the the most frequent one you see online. 

### Exercise 

* Using your `my_own_vector1` and `my_own_vector2`, compare each of these vectors using the 4 logical operators.

## Creating a tibble

A tibble is essentially a collection of columns with names, similar to an excel spreadsheet. If you are familiar with other computer languages, it is a special type of data frame that has particularly user-friendly characteristics such as making previewing data easier. To demonstrate this, we will observe the `titanic` data set we were working with earlier. Enter and run the following code:

```{r, eval = F}
## this is currently a data frame - it does not have nice features to view the data or see the data types
## the code output is suppressed here to make this document shorter
titanic
```

The `titanic` data set is currently a data frame, and hence, it is difficult to view. Let's change it to a tibble:

```{r}
## changing the titanic data frame to a tibble
titanic <- tibble(titanic)
```

Now that we've changed titanic to a tibble, it has nicer previewing features. Observe what happens when we preview the data:
```{r}
## previewing the data
titanic
```

As you can see, we now have a helpful snapshot of our data that displays far nicer than when the `titanic` data set was a data frame: we can see multiple columns, we see only the first 10 rows, and we can see the data types of each column nested under the column names (for instance, `PassengerID` is an `<int>` which stands for integer).

Tibbles are what we will be working with most frequently in this course. While it is generally uncommon to manually create a tibble, it is a great exercise to get a better understanding of how they work. First, we will create another two vectors and recreate our `first_vector` with its original values:

```{r}
## reassigning original values to the first_vector
first_vector <- c(1,3,4,10)
## new vector
second_vector <- c(1,1,2,2)
```

Now let's create a tibble that has two columns. Our first column will be the values of `first_vector` and our second column will be the values of `second_vector`. We will use the `tibble` function which takes vectors as arguments. Similarly to vectors, we need to make sure that we **save our tibble** by assigning it to a name. 

```{r}
## creating a tibble named first_tibble with two columns
first_tibble <- tibble(first_vector, second_vector)

```

Since we assigned this tibble a name, we can now see in our Global Environment that it has saved with our desired name. If we click `first_tibble` in the Global Environment, it will give us a spreadsheet view of our tibble. We can also accomplish this by using the `View` function.

```{r, eval = F}
## Viewing our tibble in spreadsheet form
View(first_tibble)
```

Notice that the tibble has two columns which are named identically to our vectors. As shown earlier using the `titanic` data, we can also perform actions on this tibble.

```{r}
## finding the mean of the first_vector column
## I did not need to set the na.rm = T since no NA values, but did it anyways
mean(first_tibble$first_vector, na.rm = T)
```

Alternatively, we could create a tibble with column names that differ from our vector names. In the previous example, our `first_tibble` has two columns with the names defaulted to `first_vector` and `second_vector`. However, we can initialize different names to the column by adding a little more syntax to our `tibble` function. Suppose I want to make a new tibble named `second_tibble` with the `first_vector` and `second_vector` as columns. However, I want the `first_vector` column to be named `age` and the `second_vector` column to be named `gender`. 

```{r}
## Creating a new tibble with two columns with the names "age" and "gender"
second_tibble <- tibble("age" = first_vector, "gender" = second_vector)
## showing the result
second_tibble
```

**One other important point**: Notice how in the `tibble` function we used `=` rather than `<-` for assigning names. It is important that you use `=` for assigning arguments rather than `<-`. Try using `<-` in the tibble argument, and see how it fails to properly do what we want. 

## Exercise 

* Using your vectors `my_own_vector1` and `my_own_vector2` that you created in the earlier exercise, create a tibble using the `tibble` function. 


## Selected Solutions

* (Exercise 1.1.1) Standard deviation: `r sd(titanic$Age, na.rm = T)`  Variance: `r var(titanic$Age, na.rm= T)`


<!--chapter:end:intro_r.Rmd-->


```{r, include=FALSE}

library(pacman)
pacman::p_load(
  "tidyverse",
  "modelsummary",
  "formatR" # used to wrap code output
)

```


# The `Rproject`

This chapter provides an introduction to the `R Project` - an organizational tool which we highly recommend you adopt ASAP. This tool will be invaluable for reproducible research, collaboration, and integrates seamlessly with Git/Github. 

When working with coauthors, each writer will need to source scripts between local machines. A common problem is that these scripts will no longer work since the file paths are "hard-coded". For example, if your Homework 1 was part of a research project, the naive start may be something like this:

```{r, eval=FALSE, echo=TRUE}
library(tidyverse)

## sets the working directory
## never use this command ever again!!!!
setwd("user/Desktop/econ_145/homework_1/")

```

If you were to send your script to your coauthor, it would not necessarily run on their local machine. It may be the case that you each have different files within different directories. This is a common problem, but there is one great solution: `R Projects`.

An `R Project` is a way to locally source all files regardless of computer you are operating on. An `R Project` automatically detects the file-path leading up to the project meaning you only have to locally source. For instance, say you created a research project folder named "research_project" on your Desktop where you keep all your files. To run a file named "regressions.R" *without* an `Rproject`, you would need to call the following using the `source` function:

```{r, eval = F, echo = T}
## The source function simply runs the file that is passed to it
## note that this file path is "hard-coded" and will not work on any other machine than the one it was created on
source("user/Desktop/research_project/regressions.R")
```

However, with an `Rproject` you could simply do the following:

```{r, eval = F, echo = T}
## this code would be able to run on any machine that has the R project
source("regressions.R")
```

Hence, as specified earlier, an `R Project` detects the file-path leading to the project folder. So in this case, every file we source will be relative to the `user/Desktop/research_project/`. This is terrific because `R Projects` will automatically detect the file-path leading to the folder, so you can send the `R Project` folder to any coauthor and they will be able to run the files on their local machine *without making any changes*. 

As a secondary example, suppose you want to load in the data which is nested in the following path: "user/Desktop/research_project/data/my_data.csv". If an `Rproject` was made in the "research_project" folder, then you would call in the data using the following:

```{r, eval = F, echo = T}
## reading in the data 
my_data <- read_csv("data/my_data.csv")
```

This is a much less error-prone way to work collaboratively, and sets you up for success for reproducibility. 

*Aside: If you're less comfortable writing file-paths, you can use the `here` package to make the job a bit easier. `here` finds the file-path leading to the `Rproject`, then allows you to enter each folder separating the lines with a comma.*

Creating an `R Project` is simple. Just click `File->New Project` in the RStudio user interface (see Figure \ref{makeproject}):

```{r, echo = F, fig.cap="\\label{makeproject}Creating a Project", out.width="75%", fig.align="center"}
# here::here() is generating the filepath such that it'll work regardless of changes in earlier folder structure or the computer it's on!
knitr::include_graphics(here::here("images","rproject","pic_1.png"))
```

RStudio also allows you to quickly switch between your projects. Figure \ref{switching} points out the `R Project` in the top right hand corner of the ``R` session. If you click on the name, the drop down appears with all of the previously accessed projects to quickly hop between projects.

```{r switching, echo = F, fig.cap="\\label{switching} Easily Switching Projects", out.width="75%",fig.align="center"}
knitr::include_graphics(here::here("images","rproject","switching.png"))
```

The other benefit of `R Projects` is the seamless synchronization and integration with Github, which will be covered soon.

This is a brief introduction to `Rprojects`. Additional reading on the matter can be found [here](https://www.r-bloggers.com/2020/01/rstudio-projects-and-working-directories-a-beginners-guide/). We highly recommend you adopt `R Projects` from the very beginning of this course - it will save you a ton of time later on!

<!--chapter:end:guided_exercise_rprojects.Rmd-->


```{r, echo = F}
library(here)
```

# Git and Github

Git is a version control system. Think of it as a better Dropbox - you can track changes of your files and revert back to old versions of projects, but all without pinging your internet every microsecond. Why not just use Dropbox? I will not explicitly tell you why you should not use certain software (by all means, use what works for you), but I will tell you some of the benefits of Git:

* It allows you to revert to previous versions of your work.
* Most private sector jobs we will be qualified for use a combination of Git and Github.
* It sends a signal to employers (and other academics) that you have ability to learn new practical skills.
* It is excellent for collaborating.
* It integrates seamlessly with RStudio.
* No more `paper_v1.pdf`, `paper_v2.pdf`, `paper_v2_edit.pdf` etc.

Unfortunately, Git has a steep learning curve. This is mainly due to the jargon that comes along with it.^[Git was created by computer scientists, for computer scientists. So the jargon doesn't always translate well to economists.] I will do my best to define terms, and hopefully make the process a little less confusing.

The next few subsections are dedicated to getting all of software installed and communicating with each other for integration. Honestly, this is an extremely painful process without good resources. Fortunately, [there is a good one](https://happygitwithr.com/install-r-rstudio.html) made by Jenny Bryan that I will be following closely throughout this document. 

## Installing Git 

Git is much different from Github. Git is the actual version control software, while Github is hosting service that provide a home for your Git-based projects. Think of Github as an online repository of files that Git can communicate with to make changes. Before we can get to Github, we need to install the Git software on our local machine. But first, let's check if you installed it at one point already (all of us probably tried learning at one point). Open up the terminal in RStudio (Tools -> Terminal -> New Terminal). You can see an example of where to type this in Figure \ref{terminal}.
```{r, echo = F, fig.cap = "\\label{terminal}The terminal in RStudio.", out.width="110%", fig.pos = "h"}
knitr::include_graphics(here::here("images", "git","terminal.png"))
```


Now type in the following:

```{bash}
which git
```

Did you get output showing you a file path similar to the one shown above? Great! You have Git installed and are ready to move on to the next section. If not, you should see something like `git: command not found` and you will need to install Git. Here is where to download:

Windows:

* https://gitforwindows.org/

Mac:

* http://git-scm.com/downloads

## Create a Github account

Once you have installed Git, it is time to make a Github account. Go [here](https://github.com/), and create an account. **Give your username some thought** since it can be a pain to change in the future. I recommend incorporating your actual name into your username. Why? Github is a website many professionals use and it's important to associate yourself with your work. Additionally, you can create your own website and host it for free using Github, but the url will be something like `yourusername.github.io`. Hence, choose your username wisely, as it will be how many access your research and materials for the foreseeable future. Also, **I recommend against using your UCSB email for your Github account**. This email will disappear when you graduate. There is no need for a headache 5 years from now. 

## Introduce yourself to Git

We are going to use the `usethis` package to do this. Type the following:

```{r, eval = F}
library(usethis)
use_git_config(user.name = "Jane Doe", user.email = "jane@example.org")
```

**Substitute your name and the email associated with your Github account!!!!**. 

## Create a PAT

*This section is almost copied entirely from [Jenny Bryan's](https://happygitwithr.com/credential-caching.html) book.*

A PAT is a personal access token. Think of it as a special password. Nowadays (as of August 2021), you need to have a PAT to be able to do the workflow proposed in this document. 

Github offers instructions for creating a personal access token and I suggest you read them. You can also type in the following:

```{r, eval = F}
usethis::create_github_token()
```

This function  take you to the web form to create a PAT with the added benefit that it pre-selects the recommended scopes.

Once you have created your PAT, install the `gitcreds` package and run the following lines (following the prompts returned by the function output):

```{r, eval = F}
## Respond to the prompt with your PAT you created
gitcreds::gitcreds_set()
```


```{r, eval = F}
## checks whether you've stored a credential
gitcreds::gitcreds_get()
```

## The Workflow

### Definitions

Before we get into the workflow, there is a lot of jargon that needs to be defined. Here are some of the main Git commands, explained in layman terms:

* **Clone** - make a copy. You will generally only need to use this command once at the start of every project.
* **Stage** - get ready to save a new Git version of a file (or files). 
* **Commit** - save the changes.
* **Push** - send the new changes. 
* **Pull** - "download" any updates.

Note that all of these commands are baked into RStudio's user interface and therefore we will not actually have to type any of these commands (although you can!). These five commands are the essence of using Git. 

### The Workflow 

*This section is to be covered thoroughly in the lecture.*

This section will cover the workflow from the start of a new project^[By project, I do not mean RProject.] to the general day-to-day tasks. Here are the steps:

1. Create a Github Repository with an intuitive name. Initialize the Github repository with a Readme, and keep all the other defaults the same. This only needs to be done one time for each project.
2. Clone the Github Repository in RStudio. This requires going to the main page of your Github repository and then:
    
    a) Clicking the green CODE button, 
    b) Copying the HTTPS to your clipboard
    c) Going to RStudio and clicking File -> New Project -> Version Control -> Git 
    d) Copying the HTTPS code into the Repository URL space, naming the RProject the same as your Github repository^[This is not necessary, you can name it whatever you want, but why confuse yourself?], and choosing the folder directory on your computer that you want this project to be nested in. This only needs to be done one time for each project.

After these two steps are done, the workflow will remain as follows *for the rest of your project*.

3. Pull any changes. It's important to make this a habit as the first thing you do when you open your RStudio project.
4. Edit files as necessary. When done making changes, stage them, commit them (adding in a nice message so you know what you did), and then push them. I will cover each of these thoroughly in further subsections.
5. Pull again. This is mostly as insurance in case you forget to pull in a future step 3. Do it. It will not hurt you and will save you headaches down the road.

In the next few subsections, I will demonstrate how to recreate this workflow.

### The Git panel of RStudio

Assuming you have cloned your repository correctly, you should now have a Git panel whenever you switch to your RStudio project that is connected with Git/Github. Figure \ref{gitpanel} shows what each of the aspects of the Git panel means: the panel shows any changes that have been made (checked if staged, unchecked if unstaged), the branch you are on (we are on the *master* in this case-more on branches later), and buttons to commit, pull, and push.

```{r, echo = F, fig.cap = "\\label{gitpanel}The Git panel of RStudio.", out.width="110%", fig.pos = "h"}
knitr::include_graphics((here::here("images", "git","stage.png")))
```

### Pulling

As stated in number 3 of the workflow, you want to press the Pull button before you make any changes. In fact, you should make a habit of this being the first thing you do the second you open an RStudio project. Note that you should also press this button after any commits you make. You can never pull too much. It is safe to continue on your work when you get the message "Already up to date."

### Commiting

Recall that "committing" is jargon for submitting a new version of your work. Git and Github will track each commit you make and you can revert back to these specific versions at any time you like. This subsection will focus on committing within RStudio. As shown in Figure \ref{gitpanel}, you can press the Commit button to save the version of your work. This will bring up a new interface in RStudio as shown in Figure \ref{commit}. From here, you are able to click on any of the files that have **saved** changes to.^[If you have not saved the changes to your local computer, Git will not show any changes!] Check any of the boxes you want to add to this particular commit. Checking a box is equivalent to "staging" a change. When you stage a change, you can see the differences in the files highlighted by green and red. You can stage as many files as you like in a commit. Personally, I think each commit should be a single task, so do not try to stage and commit too many changes at once. You should also add a commit message that explains what you did in the changes. This will be extremely useful for when you want to go back to a previous version. Click the Commit button when ready to commit to the changes.

```{r, echo = F, fig.cap = "\\label{commit}The Commit user-interface of RStudio.", out.width="110%", fig.pos = "h"}
knitr::include_graphics((here::here("images", "git","commit.png")))
```

### Pushing

Now that you have committed a change, be sure to push the changes using the Push button (either in the Git panel or Commit UI). Recall that pushing will send the changes to Github.

### Pulling (again)

Now click on the Pull button in the Git panel. If you are working alone on the master branch (we will talk about branches very soon I promise), then you should get a message saying "Already up to date." This is the message we want. Before you do anything new, you need to make certain that you are up to date.

### The Main Commands (Command Line/Terminal)

The workflow described above can all be done within the terminal. I advise against using the terminal^[We are actually going to have to use the terminal later in this documenet to revert to previous versions. Of course, you hopefully won't have to do this too often so you can simply copy and paste the lines shown later on.] as a beginner since the terminal itself has a steep learning curve. However, once (or if) you are comfortable using it, it can speed up your interactions with Git/Github. I have mapped out the main commands to get you started in the terminal if you want to try it out. Personally, I was not comfortable using these commands until a few days into writing this lecture, so no rush.

*Assuming all of this is done on the master branch. *

* *Stage changes* (e.g., get ready for saving all the changes to a version)
```{bash, eval = F}
git add -A
```

* *Commit changes* (e.g., finalize adding these changes to your version)
```{bash, eval = F}
git commit -m "this is a commit to take the staged changes and save them as a version"
```

* *Push changes* (e.g., put the new changes onto Github)
```{bash, eval = F}
git push
```

### Exercise

Make a change to your readme file. Save, stage, commit, push, and pull. Go to your Github Repository - do you see the change?

## Branches

Branches are one of the most appealing features of Git. As an example, suppose you want to try a whole new analysis in your paper, but do not necessarily want to "commit" (bad pun) to the changes. This is where branches come in. A branch is essentially a clone of your files, but you can "branch" off into different directions without hurting your master copy. This includes adding new documents, deleting documents, adding data, editing scripts etc. However, if you like the changes you make on the new branch, you can push and merge them into the master copy. Don't like the changes? Then just delete the branch and switch back to the master copy. For the purposes of this document, we will call our main copy of our files the *master branch*. This is generally the default on Github^[Sometimes Github will set up the master branch with the name *main*. You can of course change this when initializing your repository.], and you should either adopt this convention or find a very intuitive substitute. 

Branches have a steep learning curve (as does all of Git), but they are extremely important and make Git worthwhile. 

### Creating a branch

To create a branch, you want to:

1. Go to the Git panel of RStudio then click the purple L in the upper right hand corner as shown in Figure \ref{branch}. 
2. Name your branch something intuitive so you can remember what it is you wanted to accomplish in the branch. Leave all of the other presets the defaults (e.g., remote = origin, and sync branch with origin). 

Once you create your branch, you should see the label to the left of the purple L has changed to the branch you just created.
```{r, echo = F, out.width = "110%", fig.cap = "\\label{branch}Creating a branch."}
knitr::include_graphics((here::here("images", "git", "branch.png"))) 
```


### Branches when working independently

As stated before, branches are valuable for independent work when you want to try out new ideas or new paths in your analysis. For instance, suppose you want to try out some triple differences in your paper. You can create a new branch titled "analysis_triple_difference", create new documents, new tables, new R scripts etc. If you want these to be saved on Github (you do), stage, commit, and push the changes you made in the branch. Remember, the branch is effectively a copy of the master with you "branching off" into new directions. All of your work is going to be saved locally and remotely when you commit and push the changes, but it will not be part of the master branch until you push and "create a pull request" on Github (more on this in a second).

Hence, let's go through the two scenarios: (1) you like your changes you made on your branch and want to merge them into the master branch (2) you do not like your changes and want your files to be back to how they were.

1. Now that you like your changes you can create a pull request on Github. Go to your Github account on your web browser and navigate to the appropriate repository. You should that there is a "Compare & pull request" button that shows up at the top of your repository - click there (see Figure \ref{pullrequest}). From here you can "Create a pull request". A pull request is a request to merge in the new changes you made to the master branch. Click "Create a pull request". Now, assuming the branch has no conflicts with your master branch, you can click "Merge pull request". This will merge your changes into the master branch. Switch back to the master branch in RStudio, and pull down the changes. 

2. If you made changes on a branch and you don't like them, you can simply switch back to the master branch and all of your changes will be gone. Of course, your branch still exists, and that may be irritating to you (it is to me). However, you can delete the branch on your local machine by typing `git branch -D branch_name` in the terminal, and you can delete your branch on Github using point-and-click by going to Code -> Branches -> clicking on the trash can next to the branch you don't want anymore. 

```{r, echo = F, out.width = "110%", fig.cap = "\\label{pullrequest}Creating a pull request."}
knitr::include_graphics((here::here("images", "git","pullrequest.png")))
```

### Branches when working collaboratively

Consider the following workflow which is meant to demonstrate the problem of working on only one branch on a collaborative project:

> *You and your coauthor both decide to work on the R script titled `regressions_main.R` at 10:00am. You want to try one analysis, while your coauthor wants to try another. At 10:30am, you find some unique results that you believe really enhance your paper. You save the file, commit the changes, and (try to) push to Github. However, 5 minutes before you, your coauthor found different results that they believe are worthy of saving. They saved, committed, and pushed to Github at 10:25am. Now when you try and push your changes, there is an issue. Git had already saved the version that your coauthor pushed at 10:25, so it does not know how to merge in your changes with the new lines your coauthor made.*

Branches solve this problem. Each person can make their own branch which is a copy of the master branch (call this branch_objective), and begin working on their changes. Essentially every task you do when working collaboratively should be done on a branch with each author first pulling any new changes on the master branch, creating a branch that is a copy of the master, adding changes, committing and pushing the changes, then creating a pull request. 

A rather nice feature of pull requests is that you can set up your coauthor to be a reviewer of the code you wrote. This will allow your coauthor to sign off on the final changes before merging them with the master branch.


### Branches with the Terminal

* Make a new branch and switch to it (note that the "checkout" command means "switch to"):
```{bash, eval = F}
git checkout -b "branch_name"
```

* Push the branch to Github so it will sync.
```{bash, eval = F}
git push -u origin branch_name
```

Now you can continue making changes as you did before. One new thing will be that you will need to go to your Github account and merge the changes you made by "creating a pull request". 

* Deleting a branch so it no longer appears in your branches on RStudio:
```{bash, eval = F}
git branch -D branch_name
```


## .gitignore

When you cloned the Github repository and synced your RStudio project, you should notice in the project folder that there is a file called `.gitignore`. This is an important file. It tells Git what to...ignore. When should you make edits to this? I find that it is best to ignore large data files as Git was primarily made for tracking text changes. Additionally, I always add the line `**/.DS_Store` to the `.gitignore`. These are files that are unnecessary that get updated when compiling documents in RStudio. It's best to ignore these.  

You can check out other nice patterns for ignoring files using the [git documentation](https://git-scm.com/docs/gitignore). 

## Reverting 

Of course, the main benefit of using Git is that you can revert back to previous versions of your work. In this section, I will cover how to revert back to a previous version using a "safe" method.

### Reverting Safelty

*I want to preface these next few steps by saying if you are working collaboratively, you and your coauthor should make sure to have each others master branches fully pulled and up to date before doing anything of this sort. Have only one author do these steps. However, for this Guided Exercise, we will be working alone.*

First, create a new .R file and title it `one.R`. Save, stage, commit (with message "get back here"), push, and pull. Now, let's create 2 new .R files - `two.R` and `three.R`, adding in some rubbish text to each. Save stage, commit (with message "do not want"), push and pull. 

Next, let's try to get back to our previous version where we only had `one.R` in our file directory. To do this, we are going to need to use both branches and the terminal, as RStudio's UI does not have this functionality. Open the Terminal in RStudio by going to Tools -> Terminal -> New Terminal. 

Go to your Github repository on a web browser and click on the "commits" button. Here, we can see all of the commits we've made to the repository with numbers associated with each commit. For fun, you can click on any of these commits and browse what the repository looked like at each of these stages (time travel!). Our goal is to get back to the previous version where we only had the .R file titled `one.R`. Now we are going to follow the following steps:

1. Copy the commit code you see associated with this version (it should read something like `bf7a65c`).
2. Create a **new branch** called "revert" and switch to it.
3. Type the following in the terminal `git reset --hard bf7a65c`. This does a hard reset to the version bf7a65c.
4. Type the following in the terminal `git reset --soft Head@{1}`.
5. Type the following in the terminal `git commit -m "reverting to bf7a65c"`. This commits the version.
6. Type the following in the terminal `git push`. This pushes the old version to Github.
7. Finally go to Github and create a pull request. You should be able to merge and bring yourself back!






<!--chapter:end:git.Rmd-->


# Cleaning Data I

For this chapter, we will once again be working with the `titanic_train` data set from the `titanic` package. This data set provides information on the fate of passengers on the fatal maiden voyage of the ocean liner "Titanic" summarized according to economic status (class), sex, age, and survival. As a reminder, here are some of the important columns: 

* `Survived`: binary variable equal to 1 if the passenger survived
* `Pclass`: the passenger's class
* `Name`: the passenger's name
* `Sex`: the sex of the passenger
* `Age`: the age of the passenger

Now let's load in the data and necessary packages. 
```{r, message = F}
## install the package if you do not have it
# install.packages("titanic")
library(titanic)
library(tidyverse)
library(janitor)

## saving our data to the name titanic as a tibble
titanic <- tibble(titanic_train)
```

We will be focusing on getting comfortable with the following functions in the `tidyverse` package:

* `select`
* `distinct`
* `mutate`
* `group_by`
* `summarize`

We will also learn how to use the following functions in the `janitor` package:

* `clean_names`

We will be using these functions in conjunction with the pipe (typed as `%>%`) operator. By going through these exercises, you will see proper ways to utilize functions and learn to write code in a readable and reproducible way. 

## The Pipe

A pipe (typed as `%>%`) is a specific operator that comes from the `magrittr` package, but is automatically loaded in with the `tidyverse`. Essentially, it makes reading code easier, typing code faster, and finding  complicated results very easy. A pipe essentially is saying "and now, do this" to a tibble. Piping is best used to chain together multiple functions to subset your data set into smaller pieces that you find more interesting. We will see this pipe in action in the following sections. Mastering the pipe is essential to quick and efficient cleaning, and you can find some incredible results with using pipes and a few simple functions. To demonstrate how a pipe works, I'll use a rather bland motivating example: suppose we want to glance at our `titanic` data using the `head` function. Before the pipe existed, we would need to type the following:
```{r, eval = F}
## without a pipe
head(titanic)
```

However, with a pipe, we can type the following:
```{r, eval = F}
## using a pipe
titanic %>% 
  head()
```

Each of these give us the same result. The way the pipe works, is that it automatically fills the "data" argument of a function with the tibble you are piping from (in this case, the `titanic` tibble). While this seems extraneous in this example, the benefit of the pipe is that you can combine multiple functions together in a readable way. We will see a demonstration of this as we get further into this Guided Exercise.


## The `clean_names` function

The `clean_names` function allows us to put all of our column names in our tibble in a standardized format. In particular, the function makes certain that the column names are all lowercase and blank spaces are replaced with an underscore. Keeping your variable names standardized is an important practice that will make data wrangling much easier as your data sets get bigger and you begin collaborating with others.

Let's take a look at our data set without cleaning the names.

```{r}
## viewing a snapshot of our data set
titanic %>% 
  head()
```

Notice here we used the pipe. The pipe told us to take the `titanic` data set, and perform the `head` function to it. In terms of piping language we could specify what happened:

* Use titanic data set
* And now take the `head` of the titanic data set

Another important aspect of this output to notice is that  our columns begin with capital letters (e.g. `Class`). As stated, we can use the `clean_names` function from the janitor package to standardize the column name format to all lower case and underscores. Observe:

```{r, eval = F}
## Using the clean_names function on the titanic data
titanic %>% 
  clean_names()
```

The column names now all have our desired standardized format. Now let's try to use our previous `head` function:

```{r}
## Using the head function again
titanic %>% 
  head()
```

The columns have gone back to their normal messy ways! This is because **we failed to save the changes we made** to our titanic data set. The pipe operator will perform functions on your tibble, but it will **not** save the changes unless you explicitly tell R to do so. 

Let's use the `clean_names` function, and save the tibble with the cleaned column names.

```{r}
## saving the titanic tibble with cleaned names
titanic <- titanic %>% 
  clean_names()
## viewing the cleaned names data set as it has now been saved
titanic %>% 
  head()
```

For the rest of this guided exercise, we will be working with this tibble.

## The `select` function

The `select` function is a way to subset your data. It selects whichever variables you are particularly concerned with. For instance, suppose we were only interested in the `age` and `survived` columns of the `titanic` tibble. We could use the `select` function to observe only these columns.

```{r}
## selecting only the age and survived columns and then previewing with head
titanic %>% 
  select(age, survived) %>% 
  head()
```

There are a few things that should be noted here. First, we did not save this sub-selection of variables as a new tibble, so this is just a temporary sub-selection. Second, we performed two pipes with one pipe on each line until the ending function. This code could be read as:

* Use the `titanic` tibble
* And then `select` the `age` and `survived` columns
* And then use the `head` function to view 

Of course, if we wanted to save our sub-selection, we could easily do this by assigning it to a new tibble.

```{r}
## assigning the subselection to a new tibble
titanic_age_survived <- titanic %>% 
  select(age, survived)

## did not use head here because do not want only the first 5 rows to be saved
```

Generally, the `select` function is a great way to subset your data to focus on only the columns you are particularly concerned with. 

## The `filter` function

The `filter` function is one of the most powerful and frequently used functions when combined with a pipe. The filter function filters your data set based on some criteria you choose. For example, suppose we want to only look at children in this data set. We can filter out all of the passengers in the data set that have an age less than 18. Observe:

```{r}
titanic %>% 
  filter(age < 18) %>% 
  head()
```


Just to further our understanding, let's once again write out what this code is doing:

* Use the titanic tibble
* And then filter out only rows that have `age` equal to "Child"
* And then give the heading of the tibble

### Exercise

* Filter the rows of the `titanic` tibble such that the column `fare` column is greater than 100.

## The `distinct` function

The `distinct` function allows you to see the unique values within a specified column. For instance, suppose we wanted to know all of the unique values that are within the `pclass` column of the `titanic` tibble. We could use the `distinct` function to do this.

```{r}
## using distinct to find unique values in a column
titanic %>% 
  distinct(pclass)
```

We can see from the `distinct` function that the `pclass` column has three unique values: 1, 2, and 3 which correspond to the passengers' class. The `distinct` function can be a great way to take a look at your data and figure out what kind of values reside within specific columns.

### Exercise

* Using the `distinct` function, find the unique values of the `age` column. 


## The `mutate` function. 

The `mutate` function creates a new column in your tibble based on some computation statement. To motivate this, suppose we wanted to create a column in the `titanic` tibble that is named `adjusted_fare` which takes the `fare` column and multiplies it by the rate of inflation to get the ticket fare in today's prices. Using the `mutate` function, we could accomplish this:

```{r}
## assigning a variable the inflation rate
inflation_rate <- 27.14

## creating a new variable called adjusted_fare which will be the fare in today's dollars
titanic %>% 
  mutate(adjusted_fare = fare * inflation_rate) %>% 
  head()
```

There is actually quite a bit going on here, so it's worth noting the syntax of the `mutate` function. 

```{r, eval = F}
## mutate function syntax
titanic %>% 
  mutate(your_variable_name = some expression)
```

Recall that the tibble **will not** save with this new variable that you created unless you tell it to do so.  

Where `mutate` becomes very powerful is using in conjunction with the `ifelse` function. The `ifelse` function is a function that takes a conditional statement, and if it is TRUE, assigns a value, and if it is FALSE, assigns a different value. For instance, suppose we want to create a binary variable equal to 1 if a person is under the age of 18 and 0 if are not. Hence, we can use the `ifelse` function to create our desired binary variable. The syntax for the `ifelse` function is as follows:
```{r, eval = F}
## ifelse syntax
ifelse(a condition, value if condition is TRUE, value if condition is FALSE)
```

This will be more clear once you see it in action. Let's actually create the desired binary variable:

```{r}
titanic %>% 
  mutate(child = ifelse(age < 18, 1, 0)) %>% 
  head()
```

Since we cannot see from the preview that we actually created a binary variable, let's use the `distinct` function as a check.

```{r}
titanic %>% 
  mutate(child = ifelse(age < 18, 1, 0)) %>% 
  distinct(child)
```

**REMEMBER** the `child` variable **DID NOT** save unless you specifically tell R to do so. We will save this variable as we will use it later.

```{r}
## saving the new variable
titanic <- titanic %>% 
  mutate(child = ifelse(age < 18, 1, 0))

## observing the first 5 rows
titanic %>%
  head()
```

## The `summarize` function

The `summarize` function allows us to create statistics over columns quickly and efficiently. As a demonstration, we will be focusing on our `survived` column. Let's suppose that we wanted to know the average of `survived`. Since this is a binary variable, this would be equivalent to the proportion of people who survived the titanic. 

Now let's make a new column called `proportion_survived` which is equal to the mean of `survived`.
```{r}
## finding the average of the survived column
titanic %>% 
  summarize(proportion_survived = mean(survived, na.rm = T))
```

Take a closer look at the `summarize` function. You can think of the `summarize` function as similar to the `mutate` function as it creates a new variable equal to some summary statistic that you tell it to do. The basic syntax for the `summarize` function is as follows:

```{r, eval = F}
summarize(your_variable_name = somefunction)
```

As another example, we could find the standard deviation of the `survived` column using the summarize function.

```{r}
##finding the standard deviation of the survived column
titanic %>% 
  summarize(sd_survived = sd(survived, na.rm = T))
```

### Exercise

* Find the variance of the `survived` column using the `summarize` function. See solutions at the end of the document. 

## The `group_by` and `summarize` functions

The `group_by` and `summarize` functions work together to make computing statistics within-groups easy. Suppose you wanted to know the average rate of survival by class type on the titanic. In other words, you suspect that the survival rate differs by people of different class To do this, you want to take the average of each group. The `group_by` function will group classes together and then the `summarize` function will be able to do summary statistics on each group individually. 
```{r}
## finding the survival rate among classes
titanic %>% 
  group_by(pclass) %>% 
  summarize(survival_rate = mean(survived, na.rm = T))
```

From here you can see that the survival rate greatly varied across different classes. It appears that survival was much more prevalent for higher class people. An interesting result! Let's also review what is happening here in the language of pipes:

* Take the `titanic` data
* And now group by class
* And now summarize the survival rate by creating a column equal to the mean of the `survived` column within each class. 

You may be curious what happens when you do a `group_by` without a `summarize`. The truth is, nothing happens! R will create a grouping, but it means nothing unless you actually perform some sort of meaningful statistic on each group. 

```{r}
## using a group_by without a summarize or following function does nothing
titanic %>% 
  group_by(sex)
```

Since it is imperative to understand the `group_by` followed by the `summarize` function, try out a couple of exercises. 

## Exercise

 * Did women have a higher rate of survival than males? Find the answer to this question using the `group_by` and `summarize` functions.

### Exercise
 * Is there a difference in survival rates between women and men who were in a higher class? Using the `group_by` and `summarize` functions, find the answer to this question. Hint: put two arguments in the `group_by` function.


```{r, echo = F}
survive_gender = titanic %>% 
  group_by(sex) %>% 
  summarize(survivalrate = mean(survived, na.rm = T))
survive_female = survive_gender$survivalrate[1]
survive_male = survive_gender$survivalrate[2]

class = titanic %>% 
  group_by(sex, pclass) %>% 
  summarize(groupings = mean(survived, na.rm = T))
first_class_f = class$groupings[1]
second_class_f = class$groupings[2]
third_class_f = class$groupings[3]
first_class_m = class$groupings[4]
second_class_m = class$groupings[5]
third_class_m = class$groupings[6]
```

## Selected Solutions

* (Exercise 1.7.1) Variance = `r round(var(titanic$survived, na.rm = T),2)`
* (Exercise 1.8.1) Women had higher survival rates at `r round(survive_female,2)`. Males were at `r round(survive_male,2)`.
* (Exercise 1.8.2) Females in first, second, and third class had survivals rate of `r round(first_class_f,2)`, `r round(second_class_f,2)`, and `r round(third_class_f,2)` respectively. On the other hand, males in first, second, and third class had survival rates of `r round(first_class_m,2)`, `r round(second_class_m,2)`, and `r round(third_class_m,2)` respectively. 




<!--chapter:end:intro_piping.Rmd-->


# Data Cleaning II

For this chapter,  we will be importing  a dataset from TidyTuesday Github. Tidy Tuesday is a weekly social data project in R where users explore a new dataset each week and share their findings on Twitter with #TidyTuesday. In particular, we will be focusing on a horror movies data set from IMDB. IMDB is the world's most popular and authoritative source for movie, TV and celebrity content, designed to help fans explore the world of movies and shows and decide what to watch. This data set shows us information on horror movies that are on IMDB's website. Here are some important variables we will be working with:

* `review_rating`- the IMDB users average rating of the movie.
* `release_country` - the country the movie was released in.
* `movie_rating` - the movie's Motion Picture Association film rating system score (e.g. G, PG, PG-13)

We will be focusing on getting comfortable with the following functions:

* `count`
* `is.na`
* `arrange`
* `filter` in conjunction with logicals

Let's begin by importing in the data. To do this, we will be importing it using the `read_csv` function. Copy and paste the following link: https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-22/horror_movies.csv
and put it into a tibble called `horror_movies` using the `read_csv` function as shown below.

```{r, message = "hide"}
## install the package if you do not have it
library(tidyverse)

## loading in the data
horror_movies <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-22/horror_movies.csv")

```

Notice how easy it was to read data off of a website and into R using the `read_csv` function: all it took was copying a pasting a link. 

## The `count` function

The `count` function takes all of the unique elements in a column, and counts how many times each element occurs. For instance, say we want to find the amount of times each movie rating occurs in our data set. We can do this using the `count` function.

```{r}
## finding out how many of each rating of horror movie there are
horror_movies %>% 
  count(movie_rating)
```

The `count` function also has a helpful argument called `sort`. By default, `sort` is set to `FALSE`. This means that the `count` function will not order your results in descending order by the number of times they occur. If you want to view your elements in descending order, you can set the `sort` argument to `TRUE`. 

```{r}
## counting in descending order by setting sort = T
horror_movies %>% 
  count(movie_rating, sort = T)
```

The `sort` argument is particularly useful for spotting things like a large amount of NAs, or getting an idea of how a column is distributed.

### Exercise

* What is the most frequent review rating?


## Using the `is.na` function

The `is.na` function used to find the NAs withing a particular column. It takes one argument: the column you specify . The `is.na` function works particularly well with the `filter` function. Suppose we want to see how many NAs are in the `movie_rating` column. We can do this by `count`, and a `filter` in conjunction with the `is.na` function.

```{r}
## Looking at only the NAs
horror_movies %>% 
  count(movie_rating, sort = T) %>% 
  filter(is.na(movie_rating))
```

While this is useful, it might be even *more* useful if we filter out the NAs. Observe:

```{r}
## Filtering out the NAs
horror_movies %>% 
  count(review_rating, sort = T) %>% 
  filter(!is.na(review_rating))
```

What exactly happened here? The `!` logical is the "not" or "negating" logical. If we were to type `filter(is.na(review_rating))` we are telling R to filter all the elements inside of `review_rating` that are NA. However, if we type `filter(!is.na(review_rating))` we are telling R to NOT filter all the elements inside of `review_rating` taht are NA. Hence, we are actually filtering out the NAs in this line of code.

### Exercise

*  Make a new tibble called $\color{magenta}{\text{horrror\_movies\_NA}}$ that filters out all the NAs in the entire data set. 

## The `arrange` function

The arrange function is a simple function that simply sorts columns into ascending or descending order. For instance, suppose we sort our entire data set by which movies had the highest `review_rating`. We could do this by using the `arrange` function:
```{r}
## using the arrange function to sort the review rating from lowest to highest
horror_movies %>% 
  arrange(review_rating) %>% 
  head(10)
```

By default, the `arrange` function sorts in ascending rather than descending order. If we want to change this, we can use the `arrange` function in conjunction with the `desc` function.

```{r}
## using the arrange and desc functions to sort the review rating from highest to lowest
horror_movies %>% 
  arrange(desc(review_rating)) %>% 
  head(10)
```


## The `filter` function with logicals

As you saw last week, the `filter` function is great for subsetting your data based on a certain criteria. However, the `filter` function becomes much more powerful when used with logical operators. The three most common logical operators we use are the following:

* `!` - the "not" logical operator
* `&` - the "and" logical operator
* `|` - the "or" logical operator

We already briefly specified the `!` logical operator in the previous section, so let's focus on the `&` and `|`. The `&` operator becomes useful when we want to `filter` based on more than one true criteria. For example, suppose we want to filter out the movies that received a 9.0 review rating or higher AND was released in Canada. We would need to evaluate whether two criteria are satisfied: the statement "movies that received a 9.0 movie rating or higher"and the statement "released only in Canada". If **both** of these statements are TRUE, then they get displayed. If not, they are filtered out.

```{r}
## filtering for only movies receiving a movie rating of 9.0 or higher 
## and in the country Canada
horror_movies %>% 
  filter(review_rating >= 9.0 & release_country == "Canada")
```

Notice that now we are looking at movies that have a 9.0 or higher movie rating, and were released in Canada. We can see that there are only 2 movies that match these criteria.

On the other hand, suppose we used the `|` logical operator instead. The `|` operator will evaluate whether "review rating is great than 9.0" is TRUE, or "release country is Canada" is TRUE. If **either** of these statements are TRUE or **both** of these are TRUE, then the data is displayed. If **both** of these are false, then they are filtered out. Observe:

```{r}
horror_movies %>% 
  filter(review_rating >= 9.0 | release_country == "Canada") %>% 
  count(review_rating, release_country,  sort = T)
```

Notice that we we have review ratings that are less than 9.0, and also countries that are not Canada. This is because only **one** of our statements need to be TRUE (although, as stated, **both** can be TRUE as well).

## Exercise

* Use the `filter` function to filter the horror movies that were released only in the USA or were  "NOT RATED". Find which of these movies had the highest review rating. 

### Exercise

* Count the number of PG-13 movies that are only in Japan and USA. 


## Selected Solutions

```{r, echo = F}
## exercise 1.1.1
top_review_ratings = horror_movies %>% 
  count(review_rating, sort = T) 
```

* (Exercise 1.1.1) The most common review rating is `r top_review_ratings$n[1]` for NA, and `r top_review_ratings$n[2]` if we consider NA to not be a review rating. 

```{r, echo = F}
## Exercise 1.4.1
only_usa = horror_movies %>% 
  filter(release_country == "USA" | movie_rating == "NOT RATED") %>% 
  arrange(desc(review_rating))
```

 * (Exercise 1.4.1) Of the movies that were released in USA or had a rating of "NOT RATED" the highest review rating was a 9.8 by the movie `r only_usa$title[1]`.
 
```{r, echo = F}
## Exercise 1.4.2
only_usa_japan = horror_movies %>% 
  filter(release_country == "USA" | release_country == "Japan") %>% 
  count(movie_rating, sort = T) %>% 
  filter(movie_rating == "PG-13")
```
 
 * (Exercise 1.4.2) Of the movies that were released in USA or Japan, there were `r only_usa_japan$n` movies that were rated "PG-13". 

<!--chapter:end:piping_2.Rmd-->



# Graphics with ggplot2

In this chapter, we will be focusing on visualization with an emphasis on using `ggplot2`. While the basic R plots have lower fixed costs to begin using, they are generally less customizable, less initially pretty, and do not work as well in the *flow* of the `tidyverse` package. On the other hand, `ggplot2` has a logical flow to the graphics system. There is always a specific grammar that must be followed to make graphs, and once you understand it, making graphs becomes fun and (more or less) easy. In practice, you will learn that the essence of making graphs is Googling your questions. There is almost certainly an individual who has needed to make a graph similar to yours, and the R community has probably responded using `ggplot2`. 

For this week, we will once again be working with the `titanic_train` data from the `titanic` package. As a reminder, this data set provides information on the fate of passengers on the fatal maiden voyage of the ocean liner "Titanic," summarized according to economic status (class), sex, age, and survival. Here are some of the important columns:

* `Survived`: binary variable equal to 1 if the passenger survived
* `Pclass`: the passenger's class
* `name`: the passenger's name
* `Sex`: the sex of the passenger
* `Age`: the age of the passenger
* `Fare`: the price of the ticket the passenger paid


```{r, message = F}
## loading in the data and packages
library(tidyverse)
library(titanic)

##loading in the data set and cleaning the names
titanic <- janitor::clean_names(titanic_train)
```

## The grammar of graphics

The most important aspect to understand in `ggplot2` is the "grammar of graphics". The `ggplot2` package has its own syntax for making graphs. This syntax, while confusing at first, is extremely elegant when your graphics become more complicated. Let's start off with the basic template for making graphics:

```{r}
##The basic template
## This uses the titanic data set
## creates a histogram with the variable being the age column
ggplot(data = titanic, aes(x = age)) +
  geom_histogram() 
```

There is a LOT to unpack here, so we will go through each component thoroughly: 

* The `ggplot` function tells R that we want to make a `ggplot2` graphic. The `ggplot` function *generally* takes two arguments: the tibble you want to use in the `data` argument, and the `aes` function. The `aes` function stands for the "aesthetic mapping". The purpose of this function is to tell `ggplot2` what you want on your x and y axis. It will then take these inputs and "aesthetically map" them to the desired type of graph. 
* You should notice that there are addition signs (`+`) between these two lines of code. These addition signs can be thought of as a pipe, but for graphics. Specifically, they tell the graph "and now add on this". 
* The `geom_histogram` function is a function that specifies we want to make a histogram. All graphs in the `ggplot2` package begin with "geom" so that we can easily recognize that we are calling a specific type of graph. Other examples are a scatter plot (`geom_point`), density plot (`geom_density`), box-and-whisker plot (`geom_boxplot`), or a bar graph (`geom_bar`). 

## Adding options

As stated above, the `+` is essentially a `%>%`, but for graphics. It can be thought of verbally as "and now add this to the graph". To demonstrate this, let's use our histogram of the `age` column that we saw in the last section. Suppose we wanted to do the following:

* Edit the x axis with our own custom label
* Edit the y axis with our own custom label
* Add a title
* Make the default colors look better

This becomes rather simple to do in `ggplot2` thanks to the grammar of graphics. 
```{r}
## creating the same plot as above except with a title, and edited axis
ggplot(data = titanic, aes(x = age)) + ## use the titanic tibble, map the age column to the graph
  geom_histogram() + ## and now make a histogram of age
  xlab("My x-axis label which is age") + ## and now label the x axis
  ylab("My y axis label which is Count") + ## and now label the y axis
  labs(title = "My title") +## and now label. the label I want is the title
  theme_light() ## and now use this graphing theme to make it pretty
```

As specified in the comments, the way we would read this code is

* Make a ggplot object using the `titanic` tibble and map `age` to the x-axis
* And now add on a histogram 
* And now add on a label the x axis with `xlab`
* And now add on a label the y axis with `ylabs`
* And now add on the graph a title with `labs` and the title argument
* And now use a color scheme that is more appealing with `theme_light`

Let's try to make a few other types of graphs. As mentioned earlier, graph types usually begin with the `geom_` followed by the type of graph that it is. For instance, let's make a box-and-whisker graph (also known as a box-plot) using the `sex` and `age` columns. 

```{r}
## making a box-and-whisker plot
ggplot(data = titanic, aes(x = sex, y = age)) + ## make a ggplot plot using the titanic data and map age and sex to x and y
  geom_boxplot() + ## and now make a box plot with x axis sex and y axis age
  xlab("Sex of the passenger") + ## and now label the x axis 
  ylab("Age of the passenger") + ## and now label the y axis
  labs(title = "Distribution of ages by sex") + ## and now title the graph
  theme_light() ## and now make the graph look prettier
```

Now let's make a scatter plot using the `age` and `fare` columns:

```{r}
## making a scatter plot 
ggplot(data = titanic, aes(x = age, y = fare)) + ## make a ggplot plot using the titanic dat
  geom_point() + ## using age as x axis and fare as y axis
  xlab("Age of passenger") + ## and now label the x axis
  ylab("Ticket price paid") + ## and now label the y axis
  labs(title = "Relationship between age and ticket price") + ## and now title the graph
  theme_light() ## and now make the graph look prettier
```

As shown, it is incredibly simple to switch between different types of graphs. In fact, once you have a template of the certain options you like to add to your graph, you can simply change the `geom_` to your desired graph type. 

## Adding multiple plots together

One of the main draws of `ggplot2` is how simple it is to overlay graphs. For instance, suppose we want a plot that has two histograms, one for male age, and one for female age. We can easily do this by simply adding on a `geom_histogram` argument. 

```{r}
## making two histograms on one graph
ggplot(data = titanic, aes(x = age)) +
  geom_density(data = titanic %>% filter(sex == "male")) +
  geom_density(data = titanic %>% filter(sex == "female"))
```

Notice that the `geom_density` told ggplot to create a density graph. Also notice something new: we added in a `data` argument to `geom_density`. This can be extremely useful when making multiple plots on the same graph. In our example, we told our first density graph to use the `titanic` data, but `filter` only the males. This shows how simple it is to add in `tidyverse` to ggplot!

Let's make this graph a little "prettier". Suppose we wanted to fill these density graphs with some color so we could tell the difference between them. To do this, we will use the `fill` argument that comes standard in each `geom` graph. 

```{r}
## making two histograms on one graph and adding color using the fill argument
ggplot(data = titanic, aes(x = age)) +
  geom_density(data = titanic %>% filter(sex == "male"), fill = 'blue') +
  geom_density(data = titanic %>% filter(sex == "female"), fill = 'red')
```

Of course, this isn't so pretty since one of the densities is clearly over-powering the other. This is where another new argument `alpha` can help us. The `alpha` argument is simply a number between 0 and 1 which tells `ggplot` how transparent the color should be. Observe:

```{r}
## making two histograms on one graph and adding color using the fill argument, and alpha argument
ggplot(data = titanic, aes(x = age)) +
  geom_density(data = titanic %>% filter(sex == "male"), fill = 'blue', alpha = 0.4) +
  geom_density(data = titanic %>% filter(sex == "female"), fill = 'red', alpha = 0.4) +
  theme_light()
```

## Creating legends

Legends are automatically created for you using the `fill` argument **within the aesthetic mapping**. For instance, suppose we wanted to create a graph similar to above with two densities of `age`: one for males and one for females. We can actually accomplish this in a more compact way by adding the `fill` argument to our aesthetic mapping. The `fill` argument within `aes` specifically tells `ggplot` that you want to separate this graph by a categorical variable.  
```{r}
## making two histograms on one graph and adding a legend using the fill argument
ggplot(data = titanic, aes(x = age, fill = sex)) +
  geom_density(alpha = 0.4) +
  xlab("Age") + ## and now add on an xlabel
  ylab("Density") + ## and now add on a ylabel
  labs(title = "Age by Sex", fill = "Sex of Passenger") + ## and now give the graph a title, and 
  # rename the fill argument to "Sex of Passenger"
  theme_minimal() ## and now make the graph have pretty color scheme
  
```

Notice that within the `labs` function, we added in another argument, `fill`. This `fill` will specifically label the `fill` you called in the aesthetic mapping (`aes`). This is beneficial so you can label your legend however you want. Omit the `fill` argument in the `labs` function and see what happens for yourself.

Let's go through the "grammar of graphics" of this graph in plain English:

* Make a ggplot object and use the `titanic` tibble and map the `age` column to the graph, but do two separate "fills" (e.g., versions of the graph), one for each category of sex.
* And now add on a density plot
* And now add on a label the x axis with `xlab`
* And now add on a label the y axis with `ylabs`
* And now add on the graph a title with `labs` and the title argument, and re-label the `fill` argument with the label "Sex of Passenger"
* And now make the graph have default pretty colors with `theme_minimal`

### Exercise

* Create the following graph (HINT: use the `color` argument in the aesthetic mapping and turn `survived` into a factor using `as.factor`):

```{r, echo = F}
exercise_graph = titanic %>% 
  ggplot() +
  geom_point(aes(x = age, y = fare, color = as.factor(survived))) +
  xlab("Age of passenger") +
  ylab("Fare paid") +
  labs(title = "Age and fare and survival", color = "Survived") +
  theme_minimal()
```

```{r, echo = F, warning = F}
exercise_graph
```
 

<!--chapter:end:ggplot2.Rmd-->


# Strings

For this chapter, we will once again be importing the horror movies data set from the TidyTuesday Github. This data set contains information from IMDB on various horror movies. Some important columns we will be focusing on this week:

* `title`- the title of the movie
* `release_date`- movie release date in day-month-year format

Let's load in the data:

```{r}
## install the package if you do not have it
library(tidyverse)

## Load in the data
horror_movies <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-22/horror_movies.csv")
```


Our goal this week to get comfortable with three things:

1. Simple pattern matching with regular expressions
2. Using the `extract` function
3. Using the `separate` function

Using all of these three things together will allow you to do powerful data wrangling techniques that are valued greatly in the workforce. 

## Regular Expressions

Regular expressions are a fantastic tool that will allow you to match patterns in strings. To demonstrate their value, we will start off by making a vector of strings, and use `str_view` to show how we can match patterns using regular expressions.


```{r}
## creating a vector of strings I would like to analyze
x <-  c("Econ 145", "Hello!", "Best CLASS")
```

Let's see if we can match some basic patterns. Suppose I want to find the "o" in each component of the vector. I can do the following:

```{r}
## finding all of the o's in tthe vector x using str_view
str_view(string = x, pattern = "o")
```

Notice that we are matching (see the highlighted text) on the pattern "o" in every element of the vector. Note that there was no "o" to match on in the last element, "Best CLASS", so nothing was highlighted. 

We can take this even further by matching on a selection of numbers. Suppose we want to match the numbers "145". We can simply type in "145" into our pattern match.

```{r}
## matching on the pattern 145
str_view(string = x, pattern = "145")
```

However, these are all trivial examples. Regular expressions allow us to match on *patterns*, not just simple expressions. For example, suppose we have a vector which has multiple names of courses in the economics department:

```{r}
## creating a new vector with three string elements 
econ_classes <-  c("Econ 145", "Econ 10A", "Econ 140A")
```

Now suppose I want to pattern match only the course numbers. I can do this by using a regular expression. 

* `\d`: the regular expression pattern that matches any digit

Let's see this in action.

```{r, eval = F}
## matching incorrectly on any digit
str_view(string = econ_classes, pattern = "\d")
```

The first thing you should notice when you click enter is that an error message occurs: Error: '\d' is an unrecognized escape in character string starting ""\d". We get this error message because `\` is an escape character in the R programming language. Therefore, we need to escape the escape character by using `\\d` to match on a number. This can often be annoyingly difficult to remember, but the the error message should guide you to remember this detail. 
```{r}
## matching correctly on any digit
str_view(string = econ_classes, pattern = "\\d")
```

We managed to match on a number in every single element of the vector without actually specifying we wanted a "1". This is absolutely incredible. 

It is important to note that `\\d` will only match on one number, and by default, the first number it comes across in the string going from left to right. If we want to match on more numbers, we need to add in more `\\d`s. For instance, if we wanted to match on 2 digits, our pattern argument would be equal to `\\d\\d`.

```{r}
## matching on any two digits in each component
str_view(string = econ_classes, pattern = "\\d\\d")
```

Let's try to match on all of the course numbers. Notice that some course numbers have 2 digits, while others have 3. Let's see what happens when we try to match on 3 digits.

```{r}
## matching on exactly three digits in each component
str_view(string = econ_classes, pattern = "\\d\\d\\d")
```

We can see that we matched on *exactly* 3 digits, and failed to match on the element that only contained 2 digits. However, as you may have expected, we have ways to work around this using what we call *repetition patterns*. You can specify the number of matches you would like to make using the following:

* `{n}`: matches exactly n amount of your pattern
* `{n,}`: matches n or more
* `{,m}`: matches at most m
* `{n,m}`: matches between n and m

Hence, we can match on all the course digits by specifying how many times we would like to match the `\d` patter.

```{r}
##matches any digit 2 or more times
str_view(string = econ_classes, pattern = "\\d{2,}")
```

Success! We managed to match the `\d` pattern two or more times and have matched on the course digits. The way we would read this pattern is left to right: "match on any digit, and do this two times or more".  

### Exercise 1

* Find another way to match on the digits of the `econ_classes` vector.

## Regular Expressions Continued

Let's try matching on other types of expressions. 
```{r}
## creating a new vector for us to match patterns on
vector <- c("aaabbbccc", "dddAAAccc", "eib iii")
```

Here are a couple more useful patterns:

* `[abc]`: matches exactly one time on a, b, or c. If multiple appear, it matches on the one that comes first.
* `[^abc]`: matches exactly one time on anything except a, b, or c. 
* `\s`: matches any whitespace (e.g. space, tab, newline).

We will match on the beginning letter of each of these elements in our vector.

```{r}
##matching on the letters a d or e
str_view(string = vector, pattern = "[ade]")
```

### Exercise 2 

* Using the `vector` and `str_view`, match on the capital A and the whitespace. 

## Using `extract`

We will now focus on matching regular expressions within the context of tidying data. Let's specifically focus on the `title` and `release_date` columns. 

```{r}
## selecting only the title and release date columns in the horror movies data set
horror_movies <- horror_movies %>% 
  select(title, release_date)
## showing the head of the data set
horror_movies %>% head()
```

Notice that it looks like each movie title is followed by a whitespace, and then a parenthesis with the movie release year inside of it. For the purposes of explaining `extract`, we will focus on extracting the dates from the `title` column. 

The `extract` function extracts information you want from a column and puts it into a column of your choice. The specific arguments it has are col, into, and regex. These correspond to the column you want to extract an expression from, the column name you are going to send your extracted expression into, and the regular expression pattern you want to extract respectively. 

```{r}
## extracting the year from the 
horror_movies %>% 
  extract(col = title, into = "year", regex = "(\\d\\d\\d\\d)")
```

It is important to note that we **need to put parenthesis** around the pattern that we want to match in our `regex` argument for the match to work. Observe what happens when we do not:

```{r, eval = F}
## code does not work because no parenthesis around the regex expression
horror_movies %>% 
  extract(col = title, into = "year", regex = "\\d\\d\\d\\d")
```

This is because the parenthesis define a group of patterns that you want to match. Everything inside your parenthesis is a group. For the purposes of this class, we will only be extracting 1 group, as extracting more than one group gets very complicated. 

```{r}
## the correct code: works because we put parenthesis around the regex expression
horror_movies %>% 
  extract(col = title, into = "year", regex = "(\\d\\d\\d\\d)")
```

Notice when we ran the previous correct code, our `title` column disappeared. This is because by default, the extract function deletes the original column. To avoid this, we can set the `remove` argument to `FALSE`. 

```{r}
## the correct code: works because we put parenthesis around the regex expression
horror_movies %>% 
  extract(col = title, into = "year", regex = "(\\d\\d\\d\\d)", remove = F)
```

## Using `separate`

The `separate` function will split a column into multiple columns based on a regular expression. You can think of separate as a way to split with a more complicated delimiter (a delimiter is a character that separates values). For starters, we will separate once again look at the `title` column in the horror movies data. Recall that the `title` column is organized as the movie title, followed by a blank space and then the year of release in parenthesis. We will take advantage of this unique organization and separate our `title` column into two columns: `title` which will have the title of the movie, and `year` which will have the release year. While this will take multiple steps to get into a perfectly cleaned data set, we can utilize our piping procedures to make it relatively straightforward. 

First, let's separate the column based on the first parenthesis that we see.

```{r}
## separating on the first parenthesis in the title column
horror_movies %>% 
  separate(col = title, into = c("title","year"), sep = "\\(") 
```

Notice how we managed to separate the two columns, but there are now more problems we need to deal with:

1. What do the warning messages mean?
2. We need to get rid of the extra parenthesis in the `year` column.
3. We need (well, we don't NEED) to get rid of the blank space at the end of the `title` column. 

First and foremost, we will take a second to review what a warning message is. 

**Warning message:** These are messages that tell you that your R code was able to execute, but in the process, R decided to do something that you did not tell it to. It is **extremely** important that you Google warning messages, as your data could be manipulated by R in ways you really did not want. Let's look at our two warning messages in depth:
```{r, eval = F}
Warning messages:
1: Expected 2 pieces. Additional pieces discarded in 7 rows [691, 928, 1289, 2086, 2130, 2545, 3262]. 
2: Expected 2 pieces. Missing pieces filled with `NA` in 1 rows [932]
```

The first warning message is telling us that R expected 2 pieces, and then discarded additional pieces in 7 rows. While this does not make much sense at first sight, it gives us the rows which it performed this process. The best practice is to investigate these rows to get a better idea of what is happening.
```{r}
## investigating a couple of the rows that caused the warning message
horror_movies$title[691]
horror_movies$title[928]
horror_movies$title[1289]
```
By now you should see a problem: each of these warning rows have a parenthesis in the title name! This means that R is separating based on our parenthesis, but since there are multiple parenthesis, it is separating multiple times. Therefore, it is discarding the extra separated piece in each row. Generally, you want to make sure R is not throwing away any important information. You could check this yourself by saving this as a new tibble, and investigating the rows. Hence, we will create a new tibble for demonstration purposes.

```{r}
## performing the same task but saving it in a new tibble
investigate_horror_movies <- horror_movies %>% 
  separate(col = title, into = c("title","year"), sep = "\\(") 

## investigating the 691th row of the saved tibble
investigate_horror_movies[691,]

## investigating the 928th row of the saved tibble 
investigate_horror_movies[928,]
```
Clearly, this isn't giving us our desired result for the 7 rows that were causing this issue. While we will ignore this for the purpose of this exercise, you want to be very aware of what is happening to your data.

Next, let's investigate the second warning message: Expected 2 pieces. Missing pieces filled with `NA` in 1 rows [932]. It seems as though R filled a missing piece of information in our row 932. Let's take a closer look:
```{r}
## investigating what happened in row 932
## checking the title column first in our original data
horror_movies$title[932]

## look what happened to our data in row 932
investigate_horror_movies[932,]

```

Clearly we missed something: not all the movies have the same format of MOVIE TITLE (YEAR). It appears that the movie "American Exorcist" in row 932 did not have a date attached to it. Therefore, when we tried to separate by parenthesis, R was unable to perform this task and replaced the value with NA. In our case, this wouldn't have affected our analysis since there was no date attached to it, but we would want to find another way to extract the date from another column. However, as before, we will ignore this warning message since fixing it will take a lot of time in effort. It is important to realize that **you must read and investigate warning messages** since R constantly performs operations on your data that may not be exactly what you intend. 


Moving on, let's start solving our second task: getting rid of the extra parenthesis in the `year` column. Recall, we can actually use the `extract` function here to remove the final parenthesis. We can do this all in one step using a pipe.

```{r}
## separating on the first parenthesis in the title column and then extracting the four numbers for date
horror_movies %>% 
  separate(col = title, into = c("title","year"), sep = "\\(") %>% 
  extract(col = year, into = "year", regex = "(\\d\\d\\d\\d)") 
```

Finally, let's satisfy (2) and get rid of the whitespace that is at the end of our `title` column. Luckily, the `stringr` package has a pre-programmed function called `stringr::str_trim` that will trim the whitespace off the end of our column. Now we can perform the entire cleaning of the `title` column in one piping procedure:

```{r}
## separating on the first parenthesis in the title column and then extracting the four numbers for date
## and then removing the whitespace that is left over
horror_movies %>% 
  separate(col = title, into = c("title","year"), sep = "\\(") %>% 
  extract(col = year, into = "year", regex = "(\\d\\d\\d\\d)") %>% 
  mutate(title = str_trim(title))

```


## The `gsub` function

Many times, we want to simply replace simple patterns in columns of data sets. The `stringr::str_replace` function is a simple way to take expressions within variables and substitute them with a different result. Let's create a vector of values to demonstrate.

```{r}
## creating a vector to demonstrate gsub
money <- c("$100", "$150", "$200")
```

A common use of `gsub` is to get rid of dollar signs in columns that have monetary values. Omitting the dollar sign is essential if you want to perform any type of summary statistic. 

The three arguments in the `stringr::str_replace` function that we are concerned with are the `pattern`, `replacement`, and `string` arguments. The `pattern` argument is the regular expression we want to find in our vector, and the `replacement` argument is what we would like to replace the regular expression with. The `string` argument is the vector you want to perform the `stringr::str_replace` function on. 

```{r}
## the gsub function attempting (but unsuccessfully) trying to replace the $ with nothing
str_replace(money, "\\$", "")
```

Notice that we had to once again, use the escape characters since the dollar sign is a special characer.

```{r}
## successfully replacing the $ with nothing
str_replace(money, "\\$", "")

## saving this new result
money <- str_replace(money, "\\$", "")
```


### Exercise 3

```{r}
## use the following vector for the exercise
exercise <- c("100%", "94%", "87%")

```
 * Using `stringr::str_replace`, replace the percent signs in the `exercise` vector with nothing. Then, convert the `exercise` vector to a double using `as.double`, and find the variance.
 

 
## Solutions
 
 * Exercise 1: `str_view(string = econ_classes, pattern = "\\d{1,3}")`
 * Exercise 2:  `str_view(string = vector, pattern = "[A\\s]")`
 * Exercise 3: `var_exercise <- var(as.double(gsub("\\%", "", exercise)))`. 
 

 

<!--chapter:end:strings.Rmd-->




# File Organization

## Organizing 

*Writing a research project is more than economic theory, models, and analysis; it also relies on being organized. Most of us have never thought about how to organize multi-year projects because we've never had to do them. In this chapter, we will be discussing two topics on the organization side of a research project: file organization and reproducible research papers. This will be less about data cleaning and more about staying organized with `R`.*

File organization is something we all need but are rarely taught. This section closely follows Princeton's Empirical Study of Conflict (ESOC) research production guide produced by Jacob N. Shapiro. It is highly recommended you read through the production guide.[^1] The guide goes through file organization as well as best coding practices. Before continuing, it needs to be noted there is no one best way to organize files. At the end of the day, it is what works best FOR YOU! This portion of the book lays out one example of file organization. It is recommended you tweak the ideas discussed in this portion of the Guided Exercise to meet your needs. Remember, file organization is meant to make your life easier.

[^1]: [This](https://martinctc.github.io/blog/rstudio-projects-and-working-directories-a-beginner's-guide/) is a ten minute guide that covers the basics of folder structure in `R`.

Before we begin, create a new `R Project` and name it whatever you want, wherever you want. After creating an `R Project`, it is time to fill it with all your work. Your base `Rproject` directory should have every file in a folder except `.RData`, `.Rhistory`, `lab_diary.txt`, the `Rproject`, `project_idea.txt` and `git_ignore` if you're syncing with Github. Below is an example `R Project` titled payments_hate_speech:

```{r, fig.cap="Project Layout", out.width="60%", fig.align="center", echo=FALSE}
knitr::include_graphics(here::here("images", "organization","pic_2.png"))
```

Note that all folders are numbers and each number is two digits. The leading zeros are to ensure that the documents are in the correct order (e.g., 01, 02, 03).

Each folder will have a specific purpose with an accompanying `README.txt` file. The `README.txt` file should provide a 1-2 sentence description of what the file is/does. If it is raw data, it should say where the data was from. If it is an `R` script that creates your main data, say that along with what the inputs and outputs are. If it is an `R` script that makes Table X for your paper, say it does that along with what it inputs/other files it depends on. Write as if you are writing for someone who has never heard of your project nor ever looked at your data. This person may very well be a reviewer. It will also be you a year after you start your project! Figure \ref{readme} provides an example of a README.txt for the folder `03_model`. Each file in the folder has a 1 sentence description, a list of the inputs to the file, and what the file outputs. Notice the local file-paths are included in the inputs and outputs so someone can follow the files. Keeping `README.txt` files are tedious but incredibility important. If done thoroughly, the data replication needed when submitting papers is already complete. You just send the `R Project`.

```{r, fig.cap="\\label{readme} README.txt Example for 03\\_model", out.width="75%", fig.align="center", echo=FALSE}
knitr::include_graphics(here::here("images", "organization","README.png"))

```

The `lab_diary.txt` is a common tool used in other fields. It's a place to record what you tried during the day. These notes often come in handy as they cut down the time you spend re-estimating the same regression because you forgot what you did it a week ago. Here is an example of a lab diary.

```{r, fig.cap="Lab Diary", out.width="75%", fig.align="center", echo=FALSE}
knitr::include_graphics(here::here("images", "organization","pic_3.png"))

```

When progressing through your research career, it's important to figure out an organization style that works best for you. After you find it, create a general template of it. Then, whenever you have a new idea, you can just copy the template and your project is already set up. No more wasting time making folders and thinking about how you want to organize your project. Figure \ref{template} shows one approach; this approach has one general folder housing all projects, then a template for the project organization. Whenever a new idea comes to mind, the template folder is copied for quick setup.

```{r, fig.cap="\\label{template} Having a Template", out.width="50%", fig.align="center", echo=FALSE}
knitr::include_graphics(here::here("images", "organization","organization1.png"))

```

## `Rmarkdown`

`Rmarkdown` is a native report compiler in `R`. It can be used to create many many things from .pdf, .docx, and .html files to interactive dashboards to websites. It's impossible to cover everything in `Rmarkdown` in the span of a quarter, let alone a lecture. So we'll focus on the basics.

`Rmarkdown` is an alternative to Latex compilers like Overleaf[^2]. That means you can do all the same fancy math and formatting as if you were in Overleaf without ever leaving `R`. The benefits of `Rmarkdown` is the idea of code integration: you can directly run code in the document. This means you can pull data in and make a graph within your file automatically. If you update some data, the graph will update whenever you recompile. You can even have code in the middle of a sentence. You can write 2+2=`r 2+2`, but instead of writing 4, you can write `` `r ` `` with 2+2 following the r and it'll solve it within line. If you're discussing descriptive statistics in the body of your work, you no longer have to worry about copying and pasting the numbers wrong. It pulls directly from the data.

[^2]: Fancy math works here too: $\mathbb{E}[Y|X]=X\beta$

Today we'll focus on the general setup of an `Rmarkdown` file and best practices we've learned for using `Rmarkdown`.

### The Anatomy of an `Rmarkdown` File

You can create an `Rmarkdown` file just as you would an `Rscript`: `File->New File-> R Markdown`. After setting the name and type of output, you'll be greeted with this file (or something quite similar):

```{r, fig.cap="Basic Rmarkdown File", out.width="75%", fig.align="center", echo=FALSE}
knitr::include_graphics(here::here("images", "organization","rmarkdown1.png"))

```

We're going to focus on the top part, known as the YAML. This is where you set all the parameters for your document. Most YAMLs include things such as title, subtitle, author, and general paper preferences. An important note about the YAML is that indentation matters. Parameters end with a colon and parameters that are indented are read as sub-parameters. Below is an example of the YAML for a working paper as of 2021.

``` {#numCode .R .numberLines}
---
title: 'Indirect Financial Effects of Deplatforming'
subtitle: "Evidence from OwenBenjaminComedy"
author: Danny Klinenberg^[Ph.D. student at University of California, Santa Barbara;
  dklinenberg@ucsb.edu. All errors, omissions, and opinions are my own.]
date: 'Last Updated: `r Sys.Date()`'
output:
  pdf_document:
    keep_tex: TRUE
    number_sections: yes
indent: yes
header-includes:
- \usepackage{amsfonts}
- \usepackage{amsthm}
- \usepackage{amsmath}
- \usepackage[english]{babel}
- \usepackage{bm}
- \usepackage{float}
- \usepackage[fontsize=12pt]{scrextend}
- \usepackage{graphicx}
- \usepackage{indentfirst}
- \usepackage[utf8]{inputenc}
- \usepackage{pdfpages}
- \usepackage[round,authoryear]{natbib}
- \usepackage{setspace}\doublespacing
- \usepackage{subfigure}
- \theoremstyle{definition}
- \newtheorem{definition}{Definition}[section]
- \newtheorem{assumption}{Assumption}
- \newtheorem{theorem}{Theorem}[section]
- \newtheorem{corollary}{Corollary}[theorem]
- \newtheorem{lemma}[theorem]{Lemma}
- \newtheorem*{remark}{Remark}
- \newcommand{\magenta}[1]{\textcolor{magenta}{#1}}
- \newcommand{\indep}{\perp \!\!\! \perp}
- \floatplacement{figure}{H}
- \bibliographystyle{plainnat}
- \pagenumbering{gobble}
- \usepackage{eso-pic,graphicx,transparent}
link-citations: yes
bibliography: references.bib
linkcolor: blue # magenta
abstract: \singlespacing I study things. 
---
```

`Rmarkdown` also uses Latex packages. You add Latex packages under `header-includes:`. They should start with a dash space then slash just as you would do in any other Latex compiler.

Notice on line 41, there is a spot for a .bib file. You can add in-line citations by writing [@_____] and normal citations as @____ where \_\_\_\_ is the reference code for the citation in your .bib file. `Rmarkdown` works with all normal bibliography software. [Zotero](https://www.zotero.org/) is a great option  because it's free, can automatically create bibtex files, and is integrated with Rmarkdown and Microsoft Word.

There are many other options for the YAML and other ways to read in the parameters (such as a header.tex file) but we can stop here for now. The end of this discussion provides resources for in-depth questions on the matter.

*Disclaimer*: Setting up the YAML is not a one and done process most projects. It usually involves adding and removing Latex packages.

### The Body of the `Rmarkdown`

The actual document is comprise of words, Latex, and code chunks. Below are some helpful tricks for writing in Rmarkdown:

-   A line beginning with one \# signifies a section header.

-   A line beginning with two \#\# signifies a subsection header (and so on and so forth).

-   Dashes at the beginning of a line signify a bullet point.

-   One star around a word a phrase will italicize it. Two will bold it.

-   You can use all your favorite Latex commands in the body just as you would any other compiler.

The main benefit to `Rmarkdown` is the code chunk. You can create code chunk by pressing `option+command+i` on a Mac. You should see the following appear within your `Rmarkdown` file:

```{r, echo=FALSE, out.width="50%", fig.cap="R code chunk", fig.align='center'}
knitr::include_graphics(here::here("images", "organization","rmarkdown2.png"))

```

A code chunk operates just like it's own little `Rscript`. Code chunks in an `Rmarkdown` file are all thought of as being part of the same `Rscript`, meaning they all use the same global environment. The top part of the code chunk, `{r}`, has a few options we'd like to highlight.

First, you can name your code chunks by adding a title after the r: \`\`\`{r example_code_chunk}. There can't be any spaces in the code chunk. This is useful for quickly moving around your document.

```{r, echo=FALSE, out.width="50%", fig.cap="\\label{quickly}Quickly Moving", fig.align='center'}
knitr::include_graphics(here::here("images", "organization","rmarkdown3.png"))

```

You can also choose what each code chunk does. To use these options, you write {r, option1=, option2=,...}. Below is a short list of the common settings you may be interested in:

-   `echo`: If you see the code in the document: echo=FALSE means you will not see the code.
-   `include`: Whether or not the output is shown in the document. `include=FALSE` means the output will not be shown.
-   `eval`: Whether or not the code is evaluated.

The code chunks will be most useful for integrating tables, images, and graphs. For example, the last image we saw was produced using the following code:

```{r,eval=FALSE}
knitr::include_graphics(here::here("images", "organization","rmarkdown2.png"))
```

Some useful options for figures and graphs are:

-   `fig.cap`: allows you to add a figure caption. You can make the figure reference-able by using label{}. For example, {r, fig.cap="\\\\label{graph1} graph 1"} will allow you to reference graph 1 in the text by using \\ref{graph1} Then the graph number in the text will always match the graph number in the title. If creating a graph natively using ggplot2/other graphing technique, you will still want to use `fig.cap` so that the numbers automatically update correctly and you can reference the graphs in text.

-   `out.width`: Allows you to change the size of the image or graph. examples would be `out.width="50%"`.

-   `fig.align`: If you want your figure to be centered, left justified, or right justified.

Finally, we can see that making tables becomes a painless process. The following code chunk creates a tibble to demonstrate how easy it is to make a table.

```{r, echo=TRUE}
random_data<-tibble(y=rnorm(100,0,1),
                    x=y+4+rnorm(100,0,1)
                    )
```

Below, a simple regression is estimated. Using one line, the estimates are pushed to a publication-ready table as shown in Table \ref{table1}.[^3]

[^3]: `modelsummary::modelsummary` is a very powerful tool. Please refer back to Homework 4 for a description of the tool and link to their introduction site.

```{r, echo=TRUE}
lm(y~x,data = random_data) %>% 
  modelsummary::modelsummary(title = "\\label{table1}A Publication-Quality Table")
```

With tables, you want to put the title in the table function. With graphs and images, you want to put the title in `fig.cap` at the top of the code chunk.

\newpage

### Childing

Research papers can be tens, if not hundreds, of pages long. No matter which compiler you choose, you will run into errors that will not let your paper compile. One way to limit this issue is to child parts of your paper into a main `Rmarkdown` file. Childing is to `Rmarkdown` as source is to `R` scripts. It allows us to tell `Rmarkdown` to go read in other `.Rmd` files. Figure \ref{files} shows a general setup for a research paper using `Rmarkdown`.

```{r, fig.cap="\\label{files}Rmarkdown Research Paper", out.width="60%",fig.align='center'}
knitr::include_graphics(here::here("images", "organization","rmarkdown_3.png"))
```

File `00_klinenberg_hatepay.Rmd` is the master file that brings all the other files in. This is where we put our YAML information and bring the other files in. Figure \ref{main_rmd} shows the beginning of the master .Rmd with the YAML collapsed form view.

```{r, fig.cap="\\label{main_rmd}Example Master Rmarkdown File", out.width="60%",fig.align='center'}
knitr::include_graphics(here::here("images", "organization","rmarkdown4.png"))
```

Notice lines 82 and 86. The `R` header has the `child` option chosen and nothing else. This means that the `R` chunk will read the file as it's execution. If you don't have all the files next to each other, make sure to properly specify the file path.

Now, if your paper is not compiling, you do not have to go through the entire paper. You can individually load each `.Rmd` file and run it in isolation. Hence, you only need to debug 1-2 pages rather than 50.

### Bibliographies

You can specify where the bibliography will go by writing the following lines in a `Rmarkdown` file:

    # References {.unnumbered}

    ::: {#refs}
    :::

You can also specify when the appendix begins (meaning you have different numbering) by writing:

```
\appendix
```

`Rmarkdown` works with all the Latex commands you already know. For example, I like to have the appendix of my papers restart page numbers at 1 and have my tables and graphs renumber starting with A1. To do that, I include the following Latex code below the YAML of my `appendix.RMD` file:


```
    \appendix
    \renewcommand{\thefigure}{A\arabic{figure}} \setcounter{figure}{0}
    \renewcommand{\thetable}{A\arabic{table}} \setcounter{table}{0}
    \renewcommand{\theequation}{A\arabic{table}} \setcounter{equation}{0}
    \setcounter{page}{1}
```
### A Final Note on `Rmarkdown`: Visual Markdown Editor

A final note is the recent addition of the visual markdown editor. This is an html interface that blends the best of the Microsoft suite and Latex. You can access this by clicking the compass in the top right corner:

```{r, fig.cap="Enter the Editor", out.width="60%",fig.align='center', echo=FALSE}
knitr::include_graphics(here::here("images", "organization","rmarkdown5.png"))
```

When you enter the visual markdown editor, you should notice a few things. First, headers look like headers. Second, we now have options at the top of the toolbar. These options include bolding, italicizing and underlining. We also have the option to use the GUI interface to make bullet points and add in citations (using \@). The citation feature integrates with all major citation softwares (like Zotero) and automatically creates a .bib file in the same folder as your `.RMD` file.

There is much, much, more to learn about `Rprojects`, organization, and `Rmarkdown`. This served as an introduction to best practices, provided useful guides, and was an overview of what can be done. Here are some recommended additional readings:

-   [Rstudio Rmarkdown Cheatsheet](https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf)

-   [Hadley Wickam's Introduction to Rmarkdown](https://r4ds.had.co.nz/r-markdown.html)

-   [Introduction to Rmarkdown by Rmarkdown](https://rmarkdown.rstudio.com/lesson-1.html)

-   [Examples using Rmarkdown](https://rmarkdown.rstudio.com/gallery.html)

-   [Yihue Xie's newest Rmarkdown cookbook](https://bookdown.org/yihui/rmarkdown-cookbook/)

If Hadley Wickam or Yihue Xie wrote it, then it's worth reading.

<!--chapter:end:organization.Rmd-->


# Programming

For this chapter, you will not be working with a particular data set. Instead, we will be focusing on basic programming practices using vectors and functions that we create ourselves.  

## Functions

Functions are an extremely useful tool. As you will see in your homework, they can provide the benefit of nearly zero marginal cost with some upfront fixed costs. Our world is a repetitive place, and minimizing the amount of time you need to repeatedly do a task should be minimized. This is where functions will help us.

To motivate this, let's consider finding the area of a circle. The formula for area of a circle is shown in equation (1). 

\begin{equation}
Area = \pi \times r^2
\end{equation}

Suppose you wanted to find the area of two circles with radii of 3 and 5 respectively. Although a simple task, it would be painfully slow to type the numbers over and over into R and return an answer. Instead, let's create a function that will compute this area for us. 

Functions in R come with the following syntax: 
```{r, eval = F}
## function_name is the function's name that we create
## function() tells R that we are creating a function
## x is an input for the function
function_name <- function(x){
  ## body to add. 
  return()
}
```
There are a few things to note here:

* `function_name` is the name we have given our function.
* `function` tells R that we are creating a function.
* `x` is an input to the function
* `return` specifies what value should return (i.e. output).

Recall that functions are like a machine: they require an input, and using that input, create an output. In this example, our input is x, but we have not specified an output. 

To provide a concrete example, we will now create a function called `area_of_circle` that will do the following: 

1. Take the radius of the circle as an input.
2. Calculate the area of the circle.
3. Return the area of the circle as an output.

Observe:
```{r}
## This function returns the area of a circle
## This function takes 1 input: the radius of the circle
area_of_circle <- function(radius){
  area <- radius^2 * pi
  return(area)
}
```

Notice how the function was constructed. First, we write comments that explain what the function does, and what its inputs are. This is a best practice so you can understand what your function does in the future. Next, we write the actual code of the function. 

Now that we have created the function, we can input any value for the radius we like and the function will return the area of the circle.

```{r}
## Calculating area of circle with radius 3
area_of_circle(radius = 3)

## Calculating area of circle with radius 5
area_of_circle(radius = 5)
```

We could also modify the `return` statement so that it gives us a little more information:

```{r}
## This function takes 1 input: the radius of the circle
## It returns the area of the circle
area_of_circle <- function(radius){
  area <- radius^2 * pi
  return(print(paste("The area of the circle with radius", radius, "is", area)))
}
```

```{r}
## Calculating area of circle with radius 3
area_of_circle(radius = 3)

## Calculating area of circle with radius 5
area_of_circle(radius = 5)
```
### Exercise

* Create a function called `miles2kilo_feet2meter` which takes two inputs, `miles` and `feet`. The function then converts the miles input to kilometers, and the feet input to meters. The function then prints out the results, specifying each. 

## For-loops

For-loops are a common type of loop that, like functions, take care of repetitive tasks. For-loops work by iterating through a specified task for a certain amount of times. Once that specified amount of times has been met, the for-loop stops. 

The general syntax for for-loops is as follows: 
```{r, eval = F}
## for-loop syntax
for (i in {some specified range}){
  ##do something
}
```

One thing to note about for loops is the indexing variable `i`. This variable `i` will be set equal to your indexed value at each stage in the loop. We will illustrate this by example:

```{r}
## a for-loop that loops through values 1 to 5 and prints the index at each stage
for (i in 1:5){
  print(i)
}
```

The for-loop here is telling R to print the value of `i` at each iteration. The for-loop lasts for 5 iterations as we specified. Notice that at each iteration, the `i` variable is updated to match the index it is looping through. For instance, `i` is assigned to 1 the first time through the loop, 2 the second time through the loop, 3 the third time etc. However, numbers are not the only thing we can loop through. We can actually loop through all of the elements in a vector.

```{r}
## a vector for demonstration
favorite_econ = c("Econ 140A", "Econ 241", "Econ 290", "Econ 145")

## looping through each element of the favorite_econ vector
for (i in favorite_econ){
  print(i)
}
```

Here, `i` is still our index, but instead, we are telling the loop to go through each element in the vector one-by-one. Hence, on the first time through the loop, the index `i` is assigned to "Econ 140A", while `i` is assigned to "Econ 241" the second time through etc. 

Now let's try a loop that is most frequently used: looping through a entire vector's indices. 
```{r}
## looping through a vectors indices 
for (i in 1:length(favorite_econ)){
  print(favorite_econ[i])
}

```

There is quite a lot to unpack here:

* The for-loop is assigning the index `i` variable to the number 1 the first time through, 2 the second time through, and continuing on until it reaches the the number 4 which is equal to the length of the `favorite_econ` vector.
* At each iteration of the loop, we print the value of `favorite_econ` at each index. For instance, `i` is equal to 1 the first time through the loop, so the first thing that will be printed is `favorite_econ[1]` which is equivalent to "Econ 140A" (check yourself!).

Another thing to note is that you do not need to have `i` as the name of the index. In fact, when you combine multiple for-loops together, it can be helpful to change your index. Observe:

```{r}
## econ_class is the index in this case
for (econ_class in favorite_econ){
  print(econ_class)
}
```

As a final example, we will modify our `area_of_circle` function. The function will take a range of numbers as an input, and collect the area of the circle using each of these numbers as a radii. In particular, our function will take two inputs: `minradius` and `maxradius`. The `minradius` argument will be the smallest radius we would like to calculate the area of the circle with, while the `maxradius` argument will be the largest. The function will calculate the area of the circle for every integer between `minradius` and `maxradius` and return a vector with all of the areas. 

```{r}
## modifying the function to collect the area of circles with radii of integers 1 through 10
area_of_circle <- function(minradius, maxradius){
  area_vector <- rep(NA, 10) #creating an empty vector of 10 NAs
  for (i in minradius:maxradius){
    area_vector[i] = pi * i^2  #calculating the area 
  }
  return(area_vector) #returning the vector of areas
}

```

```{r}
area_of_circle(minradius = 1, maxradius = 10)
```


As an aside, it is important to note that we can also loop through columns in a data frame. This is a task you will likely need to do at some point in your data wrangling career, and there isn't much great help for this online.
```{r}
library(tidyverse)
## loading in the mtcars data set from R
cars <- mtcars

## selecting only three columns for demonstration
cars <- cars %>% select(mpg, hp, cyl)

## looping through the columns and printing them out
for (i in names(cars)){
  print(cars[i])
}
```


### Exercise

* Write a function called `sum_func` that calculates the sum of all numbers within a specified range. The function will take two inputs: `minval` which is the smallest number and `maxval` which is the largest number. 

## If-else

Conditional statements are the heart of programming. The if-else statement evaluates whether a condition is `TRUE` or `FALSE` and then perform a computation based on the truth of the statement. The syntax of an if-else statement is as follows:

```{r, eval = F}

if (logical statement){
  ##perform an action if the logical statement is TRUE 
}
else{
  ## perform a different action if NOT TRUE
}
```

For example, suppose we wanted to loop a vector of random numbers in the range of 1 to 100 and find out how many of them are greater than 50. We could do this in the following way:

```{r}
## setting the seed for replication
set.seed(1992)

## creating a random sample of 100 integers in the interval 1 to 100
numbers <- sample(1:100, 100)

## counting how many numbers come out bigger than 50
count <- 0 ## setting our counter to 0
for (i in numbers){
  ## evaluating if the number is greater than 50
  if (i > 50) {
    count <- count + 1 ## adding a 1 to our counter
  }
  else{
    count <- count ## redundant, but here for example
  }
}

```

Note the `set.seed` function. This is a function that allows replication of results when using random sampling techniques. Essentially it ensures that your random sample is the same random sample every time you run the program. While the seed was set to 1992, you can set the seed to any number you like. Of course, each seed number has its own unique sample (e.g. `set.seed(1)` will give different results than `set.seed(2)`).

### Exercise

* Loop through your `numbers` vector and assign grade values to each of the numbers, and place the grades in a separate vector. For the grade values, if the number is less than 60, assign an "F", if the number is between 60 and 69 assign a "D", if the number is between 70 and 79 assign a "C", if the number is between 80 and 89 assign a "B", and if the number is between 90 and 100 assign an "A". For fun, graph a histogram of the distribution.


\newpage

## Selected Solutions

```{r}
## a solution to Exercise 1.1.1

## this function converts miles to kilometers and feet to meters
## it takes two input paramters: miles and feet
## it returns two values: the kilometers and the meters
miles2kilo_feet2meter <- function(miles, feet){
  kilometers <- miles * 1.6
  meters <- feet * 0.3
  return(print(paste("Kilometers: ", kilometers, "Meters: ", meters)))
}

```


```{r}
## a solution to Exercise 1.2.1

## This function calculates the sum of all numbers within a specified range
## two parameters: minval and maxvalue
## minval: the minimum value of the range
## maxval: the maximum value of the range
sum_func <- function(minval, maxval) {
  total = 0
  for (i in minval:maxval) {
    total = total + i
  }
  return(print(paste("The sum of all numbers is ", total)))
}

```


<!--chapter:end:programming.Rmd-->


# Relational Data

For this chapter, we will be focusing on how to find *primary keys*. A primary key is a compact way of saying "how to uniquely identify observations in your data set". To practice finding primary keys, we will be using a variety of data sets that are already preloaded in R packages. The purpose of identifying primary keys is to make merging multiple data sets together easier. Merging data works best when you can merge on keys that uniquely identify your observations. This ensures that each observation is connected to their true data in the foreign data set (i.e. the data set that it will be merged to). A failure to merge your data correctly will result in inaccurate data that will not produce any meaningful result. This is what we want to avoid. 

## Finding Primary Keys 

The most useful technique to finding primary keys involves a combination of your intuition and the `count` and `filter` functions. To start, we'll load in the `titanic_train` data from the `titanic` package.

```{r, message = F}
##loading in the necessary package
library(tidyverse)
library(titanic)
library(janitor)
## data we will be working with  
titanic <- clean_names(titanic_train)
```

Let's take a look at the first few rows of our data by using the `head` function.

```{r}
##getting a snapshot of our data
titanic %>% 
  head()
```

To uniquely identify an observation, we need a column (or combination of columns) that puts each passenger into one row. In other words, since we have individual-level data, we want to find a row that will uniquely identify each individual. Since this is our first example, this is relatively simple: `passenger_id` uniquely identifies an individual, and so does `name`. While this seems intuitive, it is important to actually check whether this is true. This is where the technique of `filter` and `count` must be applied.

We will be using the following algorithm to check whether a column (or combination of columns) uniquely identifies our observations:

* Start with one column that you believe could uniquely identify an observation
* Count all of the elements in a column
* Filter out any of them that occur more than once
* If you do not receive any observation that occur more than once, you are done, if not, you must either try another column, or try a combination of columns. Use your intuition. 

This small algorithm ensures that we are uniquely identifying each observation. It clearly displays observations that occur more than once to inform us that this column (or columns) does or does not identify a primary key. To summarize, if we run our algorithm and receive any output of observations, we failed to find a primary key and need to rethink how we identify an observation. 

Let's try out this algorithm on our two columns that we intuitively thought could identify each individual.
```{r}
## using algorithm on the passenger_id column
titanic %>% 
  count(passenger_id) %>% 
  filter(n >1)
```

Success! The `passenger_id` column does not contain multiple observations of identification. Therefore, it uniquely identifies each observation, and we could use this column to merge in data from other data sets. 

```{r}
## using algorithm on the name column
titanic %>% 
  count(name) %>% 
  filter(n >1)
```

Another success! The `name` column also does not contain multiple observations of any particular name. Therefore, it also identifies each individual, and we could also use this column to merge in data from other data sets.

On the other hand, let's test the `ticket` column as it intuitively seems like it could also uniquely identify an observation (there is usually one ticket per person). 
```{r}
## testing if ticket uniquely identifies
titanic %>% 
  count(ticket) %>% 
  filter(n >1) %>% 
  head()
```

Clearly, since we see that there are multiple duplicates of ticket numbers, we cannot uniquely identify our observations using the `ticket` column. 

Our `titanic` dataset is simple in that we have two ways to uniquely identify an observation which only included one column, but this is not always true. In the next section, we will go through a harder example which will require a collection of columns. 

## Finding Primary Keys (harder)

In the `titanic` example, finding the primary keys was pretty intuitive, and a little trivial. However, this is not always the case. Let's take a look at a data set from the `babynames` package. The `babynames` data set provides data on names given at birth from the Social Security Administration. This includes all names with at least 5 uses (sorry Elon). For our purposes, we will subset the data set since the original data is almost 2 million observations.

```{r, message = F}
## loading the necessary package
library(babynames)
## selecting only the first 20,000 observations and getting rid of one of the rows we don't need
babynames <- babynames[1:20000,] %>% 
  select(-n)
```

```{r}
## getting a preview of the data
babynames %>% 
  head()
```

As we can see, there are four columns:

* `year`- the year of the birth
* `sex` - the sex of the child
* `name` - the name of the child
* `prop` - a weighting variable we do not care about

We want to uniquely identify an observation. This is where we need intuition as well as our algorithm. Would `name` uniquely identify an observation? No. A name can appear in many different years. Observe our algorithm:
```{r}
## checking if name uniquely identifies the data 
babynames %>% 
  count(name) %>% 
  filter(n >1)
```

Clearly, we see that there are multiple times a name appears. Hence, we must use a *collection* of columns to uniquely identify (recall our algorithm). 

Our intuition should tell us that using `name` and `year` may allow us to uniquely identify our data since each name likely occurs one time within each year. As always, our algorithm can check this assumption:

```{r}
## checking if name and year uniquely identifies the data
babynames %>% 
  count(name, year) %>% 
  filter(n>1)
```

It seems we are still not getting unique observations as there are many names that appear twice. Why would this be? This is where you must think critically about your data. Notice that our algorithm has shown us that names occur a maximum of two times. This should hint at something: *these names are being used for both males and females*. Therefore, to uniquely identify an observation, we must use *three* columns: `name`, `year`, and `sex`. 

```{r}
## checking if name, year, and sex uniquely identify an observation
babynames %>% 
  count(name, year, sex) %>% 
  filter(n > 1)
```

Success! We managed to find a grouping of columns that brought each observation to one entry. Hence, if we were merging in data, we would want to use these three columns to connect the data.

### Exercise

* (Taken from R for Data Science Chapter 13.3 Exercises) Identify the primary key in the following data set from the `fueleconomy` package: `vehicles`. Remember, the primary key could be a single column or a collection of columns.

### Exercise

* (Taken from R for Data Science Chapter 13.3 Exercises) Identify the primary key in the following data set from the `Lahman` package: `Batting`. Remember, the primary key could be a single column or a collection of columns.

### Exercise

* (Taken from R for Data Science Chapter 13.3 Exercises) Identify the primary key in the following data set from the `nasaweather` package: `atmos`. Remember, the primary key could be a single column or a collection of columns.


## Selected Solutions

These are shown on the next page. Please do not look at these until you have tried the exercises. These are provided because these exercises are more difficult.


* (Exercise 1.2.1) The column `id` uniquely identifies the observations.
* (Exercise 1.2.2) The columns (`playerID`, `yearID`, `stint`) uniquely identify each observation. The columns (`playerID`,`yearID`) are not a primary key because players can play on different teams within the same year
* (Exercise 1.2.3) The primary key is (`lat`, `long`, `year`, `month`). The primary key represents the location and time that the measurement was taken.



<!--chapter:end:relational_data.Rmd-->

